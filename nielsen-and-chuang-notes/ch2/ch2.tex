\chapter{Introduction to quantum mechanics}

\section{Notes}

\subsection{Notation}

For distinct vectors in an orthonormal set, we can write $\braket*{i}{j} = \delta_{ij}$, where $\delta_{ij}$ is the Kronecker product and is 1 if $i = j$ and 0 if $i \neq j$. 

\subsection{Matrix - Linear Operator Congruence}

For a matrix to a be a linear operator, 
$$A \Big( \sum_{i} a_{i} \ket*{v_{I}} \Big) = \sum_{i} a_{i} A \ket*{v_{i}}$$
must be true. Note the LHS is the sum of vectors to which $A$ is applied which is certainly equal to the RHS. 

Now suppose $A: V \rightarrow W$ is a linear operator and that $V$ has basis $\ket*{v_{i}}, \cdots, \ket*{v_{m}}$ and $W$ has basis $\ket*{w_{i}}, \cdots, \ket*{w_{n}}$. Since we know the $k$th column of a $A$ will be its transformation of $\ket*{v_{k}}$, 
$$A \ket*{v_{j}} = \sum_{i} A_{ij} \ket*{w_{i}}$$
Note this is just saying $A \ket*{v_{j}}$ is equal to the $j$th column of $A$, and we can think of $\ket*{w_{i}}$ as the coordinates of the transformed vector. Thus, we can find the matrix representation of any linear operator by finding a matrix $A$ with entries specified by the above equation. 

\subsection{What's so special about the Pauli matrices?}

Things to look into: \begin{itemize}
\item they pop up in the Pauli equation 
\item they form a basis for the vector space of $2 \times 2$ Hermitian matrices. Hermitian matrices represent observables? Look at Wikipedia page
\end{itemize}

\subsection{What does the Completeness Relation say about matrices?}

I don't understand why $$\sum_{ij} \bra*{w_{j}} A \ket*{v_{i}} \ket*{w_{j}} \bra*{v_{i}}$$ implies $A$ has matrix element $\bra{w_{j}} A \ket*{v_{i}}$ in the $i$th column and $j$th row, with respect to input basis $\ket*{v_{i}}$ and output basis $\ket*{w_{j}}$. page 68. 


\section{Solutions}

\exercise
$$\begin{bmatrix}
1 \\
-1
\end{bmatrix} + \begin{bmatrix}
1 \\
2 
\end{bmatrix} - \begin{bmatrix}
2 \\
1
\end{bmatrix} = 0$$

\exercise
$$A = \begin{bmatrix}
0 & 1 \\
1 & 0
\end{bmatrix}$$ because $A\ket*{0}$ has coordinate 0 in $\ket*{0}$ and coordinate 1 in $\ket*{1}$. 

If we keep our input bases the same but reorder our output bases as $\ket*{1}$ and $\ket*{0}$, 
$$A = \begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}$$

\exercise 
We know 
$$A \ket*{v_{i}} = \sum_{j} A_{ji} \ket*{w_{j}}$$
and 
$$B \ket*{w_{j}} = \sum_{k} B_{kj} \ket*{x_{k}}$$

Now we can write 
$$
\begin{aligned}
BA \ket*{v_{i}} = B ( A \ket*{v_{i}} ) = B \sum_{j} A_{ji} \ket*{w_{j}} &= \sum_{j} A_{ji} (B \ket*{w_{j}}) \\
&= \sum_{j} A_{ji} \sum_{k} B_{kj} \ket*{x_{k}} \\
&= \sum_{k} \sum_{j} B_{kj} A_{ji} \ket*{x_{k}} \\
&= \sum_{k} (BA)_{ki} \ket*{x_{k}}
\end{aligned}
$$
We know $\sum_{k} (BA)_{ki}$ is the matrix representation of operator $BA$, which the preceding step says is equal to $\sum_{k} \sum_{j} B_{kj} A_{ji}$, which is the matrix multiplication $BA$. 

\exercise 
For the same input and output basis, we want some $I$ such that 
$$I\ket*{v_{j}} = \sum_{i} I_{ij} \ket*{v_{i}} = \ket*{v_{j}}$$
which means $I_{ij} = 0$ for all $i \neq j$ and 1 otherwise. 

\exercise
For $\ket*{y}, \ket*{z_{i}} \in \C^{n}$ and $\lambda_{i} \in C$, 
$$
\begin{aligned}
\Big( \ket*{y}, \sum_{i} \lambda_{i} \ket*{z_{i}} \Big) &= \ket*{y}^{*} \sum_{i} \lambda_{i} \ket*{z_{i}} \\
&= \sum_{i} \lambda_{i} \ket*{y}^{*} \ket*{z_{i}} \\
&= \Big( \sum_{i} \lambda_{i}^{*} \ket*{z_{i}}^{*} \ket*{y} \Big)^{*}
\end{aligned}
$$
The second and third equalities demonstrate linearity in the second argument and $(\ket*{y}, \ket*{z}) = (\ket*{z}, \ket*{w})^{*}$. Finally, if $\ket*{w} = (w_{1}, \cdots, w_{n})$ where $w_{i} \in \C^{n}$, then
$$(\ket*{w}, \ket*{w}) = \sum_{i} w_{i}^{*} w_{i} = \sum_{i} \abs{w_{i}}^{2}$$
which proves the non-degeneracy and non-negativity condition. 

\exercise
$$
\begin{aligned}
\Big( \sum_{i} \lambda_{i} \ket*{w_{i}}, \ket*{v} \Big) &= \Big( \ket*{v}^{*}, \sum_{i} \lambda_{i}^{*} \ket*{w_{i}}^{*} \Big)^{*} \\
&= \sum_{i} \lambda_{i}^{*} \Big( \ket*{v}^{*}, \ket*{w_{i}}^{*} \Big)^{*} \\
&= \sum_{i} \lambda_{i}^{*} \Big( \ket*{w_{i}}, \ket*{v} \Big)
\end{aligned}
$$

\exercise
$$(\ket*{w}, \ket*{v}) = \begin{bmatrix}
1 & 1
\end{bmatrix} \begin{bmatrix}
1 \\
-1
\end{bmatrix} = 1 - 1 = 0$$
To normalize, divide each vector by $\sqrt{2}$. 

\exercise
Since at each step, we divide by the norm of the vector being added, the set is orthonormal. The set is a basis because at step $i$, we add the basis vector $\ket*{w_{i}}$ but subtract out the portion that was already in $\Span( \ket*{v_{1}}, \cdots, \ket*{v_{i-1}})$, so we still end up spanning the full vector space. 

\exercise
$$\sigma_{x} = \ket*{1} \bra*{0} + \ket*{0} \bra*{1} $$
$$\sigma_{y} = i \ket*{1} \bra*{0} - i \ket*{0} \bra*{1}$$
$$\sigma_{z} = \ket*{0} \bra*{0} - \ket*{1} \bra*{1}$$

\exercise
$$
\begin{aligned}
\ket*{v_{j}} \bra*{v_{k}} &= I \ket*{v_{j}} \bra*{v_{k}} I \\
&= \sum_{a} \ket*{v_{a}} \braket*{v_{a}}{v_{j}} \sum_{b} \braket*{v_{k}}{v_{b}} \bra*{v_{b}} \\
&= \sum_{a, b} \delta_{aj} \delta_{kb} \ket*{v_{a}} \bra*{v_{b}}
\end{aligned}
$$
so the element $(\ket*{v_{j}} \bra*{v_{k}})_{ab} = \delta_{aj} \delta_{kb}$. 

\exercise
Each of the Pauli matrices has eigenvalues $\pm 1$. 

For $\sigma_{x}$, 
$$\sigma_{x+} = \begin{bmatrix}
1 \\
1
\end{bmatrix} \text{ and } \sigma_{x-} = \begin{bmatrix}
-1 \\
1
\end{bmatrix}$$

For $\sigma_{y}$, 
$$\sigma_{y+} = \begin{bmatrix}
1 \\
i
\end{bmatrix} \text{ and } \sigma_{y-} = \begin{bmatrix}
i \\
1
\end{bmatrix}$$

For $\sigma_{z}$, 
$$\sigma_{z+} = \begin{bmatrix}
1 \\
0
\end{bmatrix} \text{ and } \sigma_{z-} = \begin{bmatrix}
0 \\
1
\end{bmatrix}$$
The diagonalization easily follows. 

\exercise
The characteristic equation is $(1 - \lambda)^{2}$, so we have eigenvalue $1$. Solving $(A - 1I)\ket*{v} = 0$ gives us $\ket*{v} = \begin{bmatrix}
0 \\
1
\end{bmatrix}$. Our diagonal form should be $$A = \ket*{v} \bra*{v} = \begin{bmatrix}
0 & 0 \\
0 & 1
\end{bmatrix} \neq \begin{bmatrix}
1 & 0 \\ 
1 & 1
\end{bmatrix}$$

\exercise
$$(\ket*{w} \bra*{v})^{\dagger} = \bra*{v}^{\dagger} \ket*{w}^{dagger} = \ket*{v} \bra*{w}$$

\exercise
Since we know $(a + b)^{\dagger} = a^{\dagger} + b^{\dagger}$, so $$\Big( \sum_{i} a_{i} A_{i} \Big)^{\dagger} = \sum_{i} \Big( a_{i} A_{i} \Big)^{\dagger} = \sum_{i} \Big( a_{i}^{*} A_{i}^{\dagger} \Big)$$

\exercise
$$(\ket*{v}, A\ket*{w}) = (A^{\dagger} \ket*{v}, \ket*{w}) = \Big(\ket*{v}, (A^{\dagger})^{\dagger} \ket*{w} \Big)$$
since this holds for all $\ket*{v}, \ket*{w}$, $A = (A^{\dagger})^{\dagger}$. 

\exercise
$$P^{2} = \sum_{i} \ket*{i} \bra*{i} \sum_{j} \ket*{j} \bra*{j} = \sum_{ij} \ket*{i} \braket*{i}{j} \bra*{j} = \sum_{ij} \delta_{ij} \ket*{i} \bra*{j}$$

Intuitively, projecting some $\ket*{v} \in P $ wouldn't change $\ket*{v}$ at all. 

\exercise
Since $A$ is normal, we can diagonalize it. If the eigenvalues are all positive, then the adjoint of this diagonal matrix of eigenvalues equals itself. If $A$ is Hermitian, then so is its diagonal form. The adjoint of a diagonal matrix consists of the conjugates of its diagonal entries, so if $A = A^{\dagger}$, then the diagonal entries (eigenvalues) must all be positive. 
