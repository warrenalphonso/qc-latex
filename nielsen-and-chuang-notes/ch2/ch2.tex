\chapter{Introduction to quantum mechanics}

\section{Notes}

\subsection{Notation}

For distinct vectors in an orthonormal set, we can write $\braket*{i}{j} = \delta_{ij}$, where $\delta_{ij}$ is the Kronecker product and is 1 if $i = j$ and 0 if $i \neq j$. 

\subsection{Matrix - Linear Operator Congruence}

For a matrix to a be a linear operator, 
$$A \Big( \sum_{i} a_{i} \ket*{v_{I}} \Big) = \sum_{i} a_{i} A \ket*{v_{i}}$$
must be true. Note the LHS is the sum of vectors to which $A$ is applied which is certainly equal to the RHS. 

Now suppose $A: V \rightarrow W$ is a linear operator and that $V$ has basis $\ket*{v_{i}}, \cdots, \ket*{v_{m}}$ and $W$ has basis $\ket*{w_{i}}, \cdots, \ket*{w_{n}}$. Since we know the $k$th column of a $A$ will be its transformation of $\ket*{v_{k}}$, 
$$A \ket*{v_{j}} = \sum_{i} A_{ij} \ket*{w_{i}}$$
Note this is just saying $A \ket*{v_{j}}$ is equal to the $j$th column of $A$, and we can think of $\ket*{w_{i}}$ as the coordinates of the transformed vector. Thus, we can find the matrix representation of any linear operator by finding a matrix $A$ with entries specified by the above equation. 

\subsection{What's so special about the Pauli matrices?}

Things to look into: \begin{itemize}
\item they pop up in the Pauli equation 
\item they form a basis for the vector space of $2 \times 2$ Hermitian matrices. Hermitian matrices represent observables? Look at Wikipedia page
\end{itemize}

\subsection{What does the Completeness Relation say about matrices?}

I don't understand why $$\sum_{ij} \bra*{w_{j}} A \ket*{v_{i}} \ket*{w_{j}} \bra*{v_{i}}$$ implies $A$ has matrix element $\bra{w_{j}} A \ket*{v_{i}}$ in the $i$th column and $j$th row, with respect to input basis $\ket*{v_{i}}$ and output basis $\ket*{w_{j}}$. page 68. 


\section{Solutions}

\exercise
$$\begin{bmatrix}
1 \\
-1
\end{bmatrix} + \begin{bmatrix}
1 \\
2 
\end{bmatrix} - \begin{bmatrix}
2 \\
1
\end{bmatrix} = 0$$

\exercise
$$A = \begin{bmatrix}
0 & 1 \\
1 & 0
\end{bmatrix}$$ because $A\ket*{0}$ has coordinate 0 in $\ket*{0}$ and coordinate 1 in $\ket*{1}$. 

If we keep our input bases the same but reorder our output bases as $\ket*{1}$ and $\ket*{0}$, 
$$A = \begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}$$

\exercise 
We know 
$$A \ket*{v_{i}} = \sum_{j} A_{ji} \ket*{w_{j}}$$
and 
$$B \ket*{w_{j}} = \sum_{k} B_{kj} \ket*{x_{k}}$$

Now we can write 
$$
\begin{aligned}
BA \ket*{v_{i}} = B ( A \ket*{v_{i}} ) = B \sum_{j} A_{ji} \ket*{w_{j}} &= \sum_{j} A_{ji} (B \ket*{w_{j}}) \\
&= \sum_{j} A_{ji} \sum_{k} B_{kj} \ket*{x_{k}} \\
&= \sum_{k} \sum_{j} B_{kj} A_{ji} \ket*{x_{k}} \\
&= \sum_{k} (BA)_{ki} \ket*{x_{k}}
\end{aligned}
$$
We know $\sum_{k} (BA)_{ki}$ is the matrix representation of operator $BA$, which the preceding step says is equal to $\sum_{k} \sum_{j} B_{kj} A_{ji}$, which is the matrix multiplication $BA$. 

\exercise 
For the same input and output basis, we want some $I$ such that 
$$I\ket*{v_{j}} = \sum_{i} I_{ij} \ket*{v_{i}} = \ket*{v_{j}}$$
which means $I_{ij} = 0$ for all $i \neq j$ and 1 otherwise. 

\exercise
For $\ket*{y}, \ket*{z_{i}} \in \C^{n}$ and $\lambda_{i} \in C$, 
$$
\begin{aligned}
\Big( \ket*{y}, \sum_{i} \lambda_{i} \ket*{z_{i}} \Big) &= \ket*{y}^{*} \sum_{i} \lambda_{i} \ket*{z_{i}} \\
&= \sum_{i} \lambda_{i} \ket*{y}^{*} \ket*{z_{i}} \\
&= \Big( \sum_{i} \lambda_{i}^{*} \ket*{z_{i}}^{*} \ket*{y} \Big)^{*}
\end{aligned}
$$
The second and third equalities demonstrate linearity in the second argument and $(\ket*{y}, \ket*{z}) = (\ket*{z}, \ket*{w})^{*}$. Finally, if $\ket*{w} = (w_{1}, \cdots, w_{n})$ where $w_{i} \in \C^{n}$, then
$$(\ket*{w}, \ket*{w}) = \sum_{i} w_{i}^{*} w_{i} = \sum_{i} \abs{w_{i}}^{2}$$
which proves the non-degeneracy and non-negativity condition. 

\exercise
$$
\begin{aligned}
\Big( \sum_{i} \lambda_{i} \ket*{w_{i}}, \ket*{v} \Big) &= \Big( \ket*{v}^{*}, \sum_{i} \lambda_{i}^{*} \ket*{w_{i}}^{*} \Big)^{*} \\
&= \sum_{i} \lambda_{i}^{*} \Big( \ket*{v}^{*}, \ket*{w_{i}}^{*} \Big)^{*} \\
&= \sum_{i} \lambda_{i}^{*} \Big( \ket*{w_{i}}, \ket*{v} \Big)
\end{aligned}
$$

\exercise
$$(\ket*{w}, \ket*{v}) = \begin{bmatrix}
1 & 1
\end{bmatrix} \begin{bmatrix}
1 \\
-1
\end{bmatrix} = 1 - 1 = 0$$
To normalize, divide each vector by $\sqrt{2}$. 

\exercise
Since at each step, we divide by the norm of the vector being added, the set is orthonormal. The set is a basis because at step $i$, we add the basis vector $\ket*{w_{i}}$ but subtract out the portion that was already in $\Span( \ket*{v_{1}}, \cdots, \ket*{v_{i-1}})$, so we still end up spanning the full vector space. 

\exercise
$$\sigma_{x} = \ket*{1} \bra*{0} + \ket*{0} \bra*{1} $$
$$\sigma_{y} = i \ket*{1} \bra*{0} - i \ket*{0} \bra*{1}$$
$$\sigma_{z} = \ket*{0} \bra*{0} - \ket*{1} \bra*{1}$$

\exercise
$$
\begin{aligned}
\ket*{v_{j}} \bra*{v_{k}} &= I \ket*{v_{j}} \bra*{v_{k}} I \\
&= \sum_{a} \ket*{v_{a}} \braket*{v_{a}}{v_{j}} \sum_{b} \braket*{v_{k}}{v_{b}} \bra*{v_{b}} \\
&= \sum_{a, b} \delta_{aj} \delta_{kb} \ket*{v_{a}} \bra*{v_{b}}
\end{aligned}
$$
so the element $(\ket*{v_{j}} \bra*{v_{k}})_{ab} = \delta_{aj} \delta_{kb}$. 

\exercise
Each of the Pauli matrices has eigenvalues $\pm 1$. 

For $\sigma_{x}$, 
$$\sigma_{x+} = \begin{bmatrix}
1 \\
1
\end{bmatrix} \text{ and } \sigma_{x-} = \begin{bmatrix}
-1 \\
1
\end{bmatrix}$$

For $\sigma_{y}$, 
$$\sigma_{y+} = \begin{bmatrix}
1 \\
i
\end{bmatrix} \text{ and } \sigma_{y-} = \begin{bmatrix}
i \\
1
\end{bmatrix}$$

For $\sigma_{z}$, 
$$\sigma_{z+} = \begin{bmatrix}
1 \\
0
\end{bmatrix} \text{ and } \sigma_{z-} = \begin{bmatrix}
0 \\
1
\end{bmatrix}$$
The diagonalization easily follows. 

\exercise
The characteristic equation is $(1 - \lambda)^{2}$, so we have eigenvalue $1$. Solving $(A - 1I)\ket*{v} = 0$ gives us $\ket*{v} = \begin{bmatrix}
0 \\
1
\end{bmatrix}$. Our diagonal form should be $$A = \ket*{v} \bra*{v} = \begin{bmatrix}
0 & 0 \\
0 & 1
\end{bmatrix} \neq \begin{bmatrix}
1 & 0 \\ 
1 & 1
\end{bmatrix}$$

\exercise
$$(\ket*{w} \bra*{v})^{\dagger} = \bra*{v}^{\dagger} \ket*{w}^{dagger} = \ket*{v} \bra*{w}$$

\exercise
Since we know $(a + b)^{\dagger} = a^{\dagger} + b^{\dagger}$, so $$\Big( \sum_{i} a_{i} A_{i} \Big)^{\dagger} = \sum_{i} \Big( a_{i} A_{i} \Big)^{\dagger} = \sum_{i} \Big( a_{i}^{*} A_{i}^{\dagger} \Big)$$

\exercise
$$(\ket*{v}, A\ket*{w}) = (A^{\dagger} \ket*{v}, \ket*{w}) = \Big(\ket*{v}, (A^{\dagger})^{\dagger} \ket*{w} \Big)$$
since this holds for all $\ket*{v}, \ket*{w}$, $A = (A^{\dagger})^{\dagger}$. 

\exercise
$$P^{2} = \sum_{i} \ket*{i} \bra*{i} \sum_{j} \ket*{j} \bra*{j} = \sum_{ij} \ket*{i} \braket*{i}{j} \bra*{j} = \sum_{ij} \delta_{ij} \ket*{i} \bra*{j}$$

Intuitively, projecting some $\ket*{v} \in P $ wouldn't change $\ket*{v}$ at all. 

\exercise
Since $A$ is normal, we can diagonalize it. If the eigenvalues are all positive, then the adjoint of this diagonal matrix of eigenvalues equals itself. If $A$ is Hermitian, then so is its diagonal form. The adjoint of a diagonal matrix consists of the conjugates of its diagonal entries, so if $A = A^{\dagger}$, then the diagonal entries (eigenvalues) must all be positive. 

\exercise
For an eigenvector $\ket*{v}$, we have $A \ket*{v} = \lambda \ket*{v} \rightarrow \bra*{v} A^{\dagger} = \lambda^{*} \bra*{v}$. Multiplying these two gives us $\bra*{v} A^{\dagger} A \ket*{v} = \lambda^{*} \lambda \braket*{v}{v}$, and because $A^{\dagger} A  = I$, 
$$\norm{v}^{2} = \abs{\lambda}^{2} \, \norm{v}^{2} \rightarrow \abs{\lambda} = 1$$

\exercise
Omitted because it's just mechanical. 

\exercise
$$
\begin{aligned}
A^{'}_{ij} &= \bra*{v_{i}} A \ket*{v_{j}}\\ &= \bra*{v_{i}} U^{\dagger} U A U U^{\dagger} \ket*{v_{j}} \\
&= \sum_{a} \sum_{b} \sum_{c} \sum_{d} \braket*{v_{i}}{w_{a}} \braket*{v_{a}}{v_{b}} \bra*{w_{b}} A \ket*{w_{c}} \braket*{v_{c}}{v_{d}} \braket*{w_{d}}{v_{j}} \\
&= \sum_{a} \sum_{b} \sum_{c} \sum_{d} \delta_{ab} \delta_{cd} \braket*{v_{i}}{w_{a}} A^{''}_{bc} \braket*{w_{d}}{v_{j}} 
\end{aligned}
$$
This tells us $a = b$ and $c = d$, so 
$$A^{'}_{ij} = \sum_{a} \sum_{c} \braket*{v_{i}}{w_{a}} A^{''}_{ac} \braket*{w_{c}}{v_{j}} $$

\exercise
We will prove any Hermitian operator $M$ is diagonal with respect to some orthonormal basis $V$. 

We proceed by induction on dimension $d$ on $V$. 

\textbf{Base case: }$d = 1$

Trivially, $M$ is diagonal. 

\textbf{Inductive hypothesis: } Assume $d = n - 1$

\textbf{Inductive step: } Prove $d = n$

Let $\lambda$ be an eigenvalue of $M$, $P$ be a projection onto the $\lambda$ eigenspace, and $Q$ be $P$'s orthogonal complement. 

We know $M = (P + Q) M (P + Q)$. First, note that $QMP = \lambda QP = 0$. Now for some $\ket*{v} \in P$, $M \ket*{v} = M^{\dagger} \ket*{v} = \lambda \ket*{v}$ because $M$ is Hermitian, which means $\ket*{v}$ is in the eigenspace $\lambda$ of $M^{\dagger}$. Now we have $QM^{\dagger} P \ket*{v} = QM^{\dagger} \ket*{v} = \lambda Q \ket*{v} = 0$. Taking the adjoint of this gives us $PMQ = 0$. Now we have $M = PMP + QMQ$.

Since $PMP = \lambda P$, $QMQ$ must be nonzero also for $M = PMP + QMQ$ to hold, so they both have dimension less than $n$. Finally, $PMP = (PMP)^{\dagger} = PM^{\dagger}P$ and similarly for $QMQ$. Since they're both Hermitian, our inductive hypothesis proves the theorem. 

\exercise
For a Hermitian operator $A$, suppose $A \ket*{v} = \lambda \ket*{v}$ and $A \ket*{w} = \mu \ket*{w}$. 

Since $\bra*{v} A = \lambda \bra*{v}$, we can write 
$$\bra*{v} A^{2} \ket*{w} = \mu^{2} \bra*{v} \ket*{w} = \lambda \mu \bra*{v} \ket*{w}$$
where the first equality follows from $A^{2} \ket*{w} = \mu^{2} \ket*{w}$. Since $\lambda \neq \mu$, $\braket*{v}{w} = 0$. 

\exercise
Suppose $\ket*{v}$ is an eigenvector of $P$ with eigenvalue $\lambda$, $P \ket*{v} = \lambda \ket*{v}$. Then
$$P \ket*{v} = P^{2} \ket*{v} = \lambda^{2} \ket*{v}$$
where the first equality follows from the property $P^{2} = P$. Since $\lambda^{2} = \lambda$, $P$'s eigenvalues must be 1 or 0. 

\exercise
Let $B = \frac{A + A^{\dagger}}{2}$ and $C = \frac{A - A^{\dagger}}{2i}$. This means 
$$\bra*{v} A \ket*{v} = \bra*{v} B \ket*{v} + i \bra*{v} C \ket*{v}$$
Since we cannot have an imaginary term in a positive operator, $C = 0$, so $A = A^{\dagger}$. 

\exercise
We can write $\braket*{Av}{Av} = \braket*{A^{\dagger} A v}{v}$.
But since $\braket*{Av}{Av}$ can also be written as $\norm{Av}^{2} \geq 0$, $A^{\dagger} A$ must be positive. 

\exercise
As a tensor product:
$$\ket*{\psi}^{\otimes 2} = \frac{1}{2} \Big( \ket*{0} + \ket*{1} \Big) \otimes \Big( \ket*{0} + \ket*{1} \Big) = \frac{1}{2} \Big( \ket*{00} + \ket*{01} + \ket*{10} + \ket*{11} \Big)$$ 

As a Kronecker product: 
Since $\frac{1}{\sqrt{2}} ( \ket*{0} + \ket*{1} ) = \begin{bmatrix}
\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}}
\end{bmatrix}$, 
$$\begin{bmatrix}
\frac{1}{\sqrt{2}} \ket*{\psi} \\
\frac{1}{\sqrt{2}} \ket*{\psi}
\end{bmatrix} = \begin{bmatrix}
\frac{1}{2} \\
\frac{1}{2} \\
\frac{1}{2} \\
\frac{1}{2} 
\end{bmatrix}$$

As a tensor product: 
$$\ket*{\psi}^{\otimes 3} = \ket*{\psi} \otimes \ket*{\psi}^{\otimes 2} = \frac{1}{\sqrt{2}} \Big( \ket*{0} + \ket*{1} \Big) \otimes \frac{1}{2} \Big( \ket*{00} + \ket*{01} + \ket*{10} + \ket*{11} \Big) = \frac{1}{2 \sqrt{2}} \Big( \ket*{000} + \ket*{001} + \ket*{010} + \ket*{011} + \ket*{100} + \ket*{101} + \ket*{110} + \ket*{111} \Big)$$

As a Kronecker product: 
$$\begin{bmatrix}
\frac{1}{\sqrt{2}} \ket*{\psi}^{\otimes 2} \\
\frac{1}{\sqrt{2}} \ket*{\psi}^{\otimes 2}
\end{bmatrix} = \begin{bmatrix}
\frac{1}{2\sqrt{2}} & \frac{1}{2\sqrt{2}} & \frac{1}{2\sqrt{2}} & \frac{1}{2\sqrt{2}} & \frac{1}{2\sqrt{2}} & \frac{1}{2\sqrt{2}} & \frac{1}{2\sqrt{2}} & \frac{1}{2\sqrt{2}} 
\end{bmatrix}^{T}$$

\exercise
$$X \otimes Z = \begin{bmatrix}
0 \cdot Z & 1 \cdot Z \\
1 \cdot Z & 0 \cdot Z
\end{bmatrix} = \begin{bmatrix}
0 & 0 & 1 & 0 \\
0 & 0 & 0 & -1 \\
1 & 0 & 0 & 0 \\
0 & -1 & 0 & 0
\end{bmatrix}$$

$$I \otimes X = \begin{bmatrix}
1 \cdot X & 0 \cdot X \\
0 \cdot X & 1 \cdot X
\end{bmatrix} = \begin{bmatrix}
0 & 1 & 0 & 0 \\
1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 1 & 0
\end{bmatrix}$$

$$X \otimes I = \begin{bmatrix}
0 \cdot I & 1 \cdot I \\
1 \cdot I & 0 \cdot I 
\end{bmatrix} = \begin{bmatrix}
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0
\end{bmatrix}$$

Since $I \otimes X \neq X \otimes I$, the tensor product is not commutative. 

\exercise
Writing the Kronecker product, 
$$(A \otimes B)^{\dagger} = \begin{bmatrix}
A_{11} B & \cdots & A_{1n} B \\
\vdots & \ddots & \vdots \\
A_{n1} B & \cdots & A_{nn} B
\end{bmatrix}^{\dagger} = \begin{bmatrix}
A_{11}^{*} B^{\dagger} & \cdots & A_{n1}^{*} B^{\dagger} \\
\vdots & \ddots & \vdots \\
A_{1n}^{*} B^{\dagger} & \cdots & A_{nn}^{*} B^{\dagger}
\end{bmatrix} = A^{\dagger} \otimes B^{\dagger}$$

Proving the transpose is similar and proving the complex conjugate requires only using $B^{*}$ instead of $B^{\dagger}$. 

\exercise
Let $U_{1}$ and $U_{2}$ be unitary. 

$$(U_{1} \otimes U_{2})^{\dagger} (U_{1} \otimes U_{2}) = (U_{1}^{\dagger} \otimes U_{2}^{\dagger})(U_{1} \otimes U_{2}) = U_{1}^{\dagger} U_{1} \otimes U_{2}^{\dagger} U_{2} = I \otimes I = I$$

\exercise
Let $A = A^{\dagger}$ and $B = B^{\dagger}$. 
$$(A \otimes B)^{\dagger} = A^{\dagger} \otimes B^{\dagger} = A \otimes B$$

\exercise
Let $A$ and $B$ be positive operators. 
$$\Big( (A \otimes B)(\ket*{v} \otimes \ket*{w}), (\ket*{v} \otimes \ket*{w} \Big) = \Big( A \ket*{v} \otimes B \ket*{w}, \ket*{v} \otimes \ket*{w} \Big) = \bra*{v} A^{\dagger} \ket*{v} \bra*{w} B^{\dagger} \ket*{w} = \bra*{v} A \ket*{v} \bra*{w} B \ket*{w}$$
since we know positive operators are Hermitian. Since $\bra*{v} A \ket*{v}$ and $\bra*{w} B \ket*{w}$ are both non-negative, their product is also non-negative. 

\exercise
Let $P$ and $Q$ be projectors. Recall that if $P^{2} = P$, $P$ is a projector. 
$$(P \otimes Q)(P \otimes Q) = P^{2} \otimes Q^{2} = P \otimes Q$$

\exercise
We proceed by induction on $n$. 

\textbf{Base case: } $n = 1$

We know $$H^{\otimes 1} = H = \frac{1}{\sqrt{2}} \Big[ (\ket*{0} + \ket*{1})\bra*{0} +  (\ket*{0} - \ket*{1})\bra*{1} \Big] = \frac{1}{\sqrt{2}} \sum_{x, y \in \{ 0, 1 \}^{1}} (-1)^{x \cdot y} \ket*{x} \bra*{y}$$

\textbf{Inductive hypothesis: } Assume $n = k - 1$

$$H^{\otimes k - 1} = \frac{1}{2^{k - 1 / 2}} \sum_{x, y \in \{0, 1\}^{k-1}} (-1)^{x \cdot y} \ket*{x} \bra*{y}$$

\textbf{Inductive step: } Prove $n = k$

$$H^{\otimes k} = H \otimes H^{\otimes k - 1} = \frac{1}{2^{k / 2}} \sum_{x_{1}, y_{1} \in \{0, 1\}} (-1)^{x_{1} \cdot y_{1}} \ket*{x_{1}} \bra*{y_{1}} \qquad \otimes \sum_{x_{2}, y_{2} \in \{ 0, 1\}^{k-1}} (-1)^{x_{2} \cdot y_{2}} \ket*{x_{2}} \bra*{y_{2}}$$
Since this tensor product only flips the sign of the $H^{\otimes k - 1}$ if $x_{1} = y_{1} = 1$, it is easy to see that concatenating $x_{1}, x_{2}$ and $y_{1}, y_{2}$, would yield the dot product required to flip the signs when we want. 

$$H^{\otimes 2} = \frac{1}{\sqrt{2}} \begin{bmatrix}
H & H \\
H & -1
\end{bmatrix} = \frac{1}{2} \begin{bmatrix}
1 & 1 & 1 & 1 \\
1 & -1 & 1 & -1 \\
1 & 1 & -1 & -1 \\
1 & -1 & -1 & 1
\end{bmatrix}$$

\exercise
Let $A = \begin{bmatrix}
4 & 3 \\
3 & 4
\end{bmatrix}$. First, we find the eigenvalues and eigenvectors. 

$$\Det(A - \lambda I) = (\lambda - 7)(\lambda - 1) \rightarrow \lambda = 7, 1$$
with corresponding eigenvectors 
$$\frac{1}{\sqrt{2}} \begin{bmatrix}
1 \\
1
\end{bmatrix} \text{\qquad and \qquad} \frac{1}{\sqrt{2}}	 \begin{bmatrix}
1 \\
-1
\end{bmatrix}$$
We'll denote the eigenpairs as $(7, \ket*{a})$ and $(1, \ket*{b})$.  

So, 
$$\sqrt{A} = \sqrt{7} \ket*{a} \bra*{a} + 1 \ket*{b} \bra*{b}$$
and 
$$\log (A) = \log (7) \ket*{a} \bra*{a}$$

\exercise
Since $v \cdot \sigma$ is a weighted sum of the Pauli matrices, we know it will have eigenvalues of 1 and -1. Let the eigenpairs of $v \cdot \sigma$ be $(1, \ket*{\lambda_{1}})$ and $(-1, \ket*{\lambda_{-1}})$. 

$$
\begin{aligned}
\exp (i \theta v \cdot \sigma) &= e^{i \theta} \ket*{\lambda_{1}} \bra*{\lambda_{1}} + e^{-i \theta} \ket*{\lambda_{-1}} \bra*{\lambda_{-1}} \\
&= (\cos \theta + i \sin \theta) \ket*{\lambda_{1}} \bra*{\lambda_{1}} + (\cos \theta - i \sin \theta) \ket*{\lambda_{-1}} \bra*{\lambda_{-1}} \\
&= \cos \theta \Big( \ket*{\lambda_{1}} \bra*{\lambda_{1}} + \ket*{\lambda_{-1}} \bra*{\lambda_{-1}} \Big) + i \sin \theta \Big(\ket*{\lambda_{1}} \bra*{\lambda_{1}} - \ket*{\lambda_{-1}} \bra*{\lambda_{-1}} \Big) \\
&= \cos \theta I + i \sin \theta v \cdot \sigma 
\end{aligned}
$$