%%% DID NOT DO RIGID MOTIONS AND COMPLEXIFICATION!!
%%% DID NOT DO RIGID MOTIONS AND COMPLEXIFICATION!!
%%% DID NOT DO RIGID MOTIONS AND COMPLEXIFICATION!!

\chapter{Inner Product Spaces}

Keep in mind that theory for inner product space is only developed for $\mathbb{R}$ and $\mathbb{C}$, so $\mathbb{F}$ will always denote one of those two fields in the next two chapters. 

\section{Inner Product}

\begin{definition}
We define the \textbf{norm} of a vector to be the generalization of \textit{length}. That is, the norm of a vector $x \in \mathbb{R}^{n}$ is 
$$\norm{x} = \sqrt{x_{1}^{2} + \cdots + x_{n}^{2}}$$

For any complex number $z = x + iy$, we can write $\abs{z}^{2} = x^{2} + y^{2} = z \overline{z}$, where $\overline{z}$ denotes the complex conjugate of $z$. For any $z$ in a complex field $\mathbb{C}^{n}$, we can write 
$$z = \begin{bmatrix}
z_{1} \\
\vdots \\
z_{n}
\end{bmatrix} = \begin{bmatrix}
x_{1} + iy_{1} \\
\vdots \\
x_{n} + iy_{n}
\end{bmatrix}$$
so it is natural to define the norm $\norm{z}$ as 
$$\norm{z}^{2} = \sum_{k = 1}^{n} (x_{k}^{2} + y_{k}^{2}) = \sum_{k=1}^{n} \abs{z_{k}}^{2}$$
\end{definition}

\begin{definition}
The \textbf{inner product} of two vectors $x, y \in \mathbb{R}^{n}$ is
$$(x, y) = x_{1} y_{1} + \cdots + x_{n} y_{n} = x^{T} y = y^{T} x$$

This yields another definition for the \textbf{norm}: 
$$\norm{x} = \sqrt{(x, x)}$$

For complex fields, we need a definition of inner product such that $\norm{z}^{2} = (z, z)$. One definition that is consistent with this requirement will be our definition for the \textbf{\textit{standard} inner product in $\mathbb{C}^{n}$}, 
$$(z, w) = z_{1} \overline{w_{1}} + \cdots + z_{n} \overline{w_{n}}$$

To simplify this, we will define the \textbf{Hermitian adjoint}, or simply \textbf{adjoint} $A^{*}$, by $A^{*} = \overline{A}^{T}$. 

Using this, we can write 
$$(z, w) = w^{*} z$$ 
\end{definition}

The inner products we defined for $\mathbb{R}^{n}$ and $\mathbb{C}^{n}$	have the following properties: 
\begin{enumerate}
	\item Symmetry: $(x, y) = \overline{(y, x)}$
	\item Linearity: $(\alpha x + \beta y, z) = \alpha (x, z) + \beta (y, z)$
	\item Non-negativity: $(x, x) \geq 0$ 
	\item Non-degeneracy: $(x, x) = 0$ if and only if $x = 0$
\end{enumerate}

Note that properties 1 and 2 imply that 
$$(x, \alpha y + \beta z) = \overline{(\alpha y + \beta z, x)} = \overline{\alpha} (x, y) + \overline{\beta} (x, z)$$

\begin{lemma}
Let $x$ be a vector in $V$. Then $x = 0$ if and only if  
$$(x, y) = 0 \qquad \forall y \in V$$
\end{lemma}

\begin{lproof}
Since $(0, y) = 0$, we need to only show that $x = 0$ if $(x, y) = 0$. Subbing in $y = x$, we get $(x, x) = 0$ and non-degeneracy asserts that $x = 0$. 
\end{lproof}

\begin{lemma}
Let $x, y$ be vectors in $V$. Then $x = y$ if and only if  
$$(x, z) = (y, z) \qquad \forall z \in V$$
\end{lemma}

\begin{lproof}
Using the above lemma, if we set $(x - y, z) = 0 \: \forall z \in V$, then it follows that $x = y$ and $(x, z) = (y, z)$. 

\end{lproof}

\begin{theorem}
Suppose two operators $X, Y : A \rightarrow B$ satisfy 
$$(Ax, y) = (Bx, y) \qquad \forall x \in X, \forall y \in Y$$
Then $A = B$. 
\end{theorem}

\begin{proof}
Using the previous lemma, we can fix $x$ and take all $y \in Y$, which means $Ax = Bx$. Since this is true for all $x$, $A$ and $B$ are the same operator. 
\end{proof}

\begin{theorem}[Cauchy-Schwarz Inequality]
$$\abs{(x, y)} \leq \norm{x} \cdot \norm{y}$$
\end{theorem}

\begin{proof}
If $x$ or $y$ is 0, then the proof is trivial. Assuming neither is 0, we will prove both the real and complex cases. But first consider only the real case: 
$$0 \leq \norm{x - ty}^{2} = (x - ty, x - ty) = \norm{x}^{2} - 2t(x, y) + t^{2} \norm{y}^{2} $$
Taking the derivative with respect to $t$ and setting it to 0 gives us $t = \frac{(x, y)}{\norm{y}^{2}}$. We will use this same $t$ value for the following proof of the real and complex cases:

$$
\begin{aligned}
0 \leq \norm{x-ty}^{2} &= (x - ty, x - ty) \\ 
&= (x, x - ty) -t(y, x - ty) \\
&= \norm{x}^{2} - \overline{t} (x, y) -t (y, x) + \abs{t}^{2} \norm{y}^{2}
\end{aligned}
$$

Using symmetry of inner products, we have 
$$t = \frac{(x, y)}{\norm{y}^{2}} = \frac{\overline{(y, x)}}{\norm{y}^{2}}$$
Subbing in $t$, we get 
$$ 0 \leq \norm{x}^{2} - \frac{\abs{(x y)}^{2}}{\norm{y}^{2}}$$
which completes the proof. 
\end{proof}

\begin{theorem}[Triangle Inequality]
$$\norm{x + y} \leq \norm{x} + \norm{y}$$
\end{theorem}

\begin{proof}
$$
\begin{aligned}
\norm{x + y}^{2} = (x + y, x + y) &= \norm{x}^{2} + \norm{y}^{2} + (x, y) + (y, x) \\
&\leq \norm{x}^{2} + \norm{y}^{2} + 2 \abs{(x, y)} \\
&\leq \norm{x}^{2} + \norm{y}^{2} + 2 \norm{x} \cdot \norm{y} \\
&= (\norm{x} + \norm{y})^{2}
\end{aligned}
$$
\end{proof}

\begin{theorem}
The following \textbf{polarization identities} allow us to construct the inner product from the norm: 

For $x, y \in \mathbb{R}^{n}$, 
$$(x, y) = \frac{1}{4} \Big( \norm{x + y}^{2} - \norm{x - y}^{2} \Big) $$

For $x, y \in \mathbb{C}^{n}$, 
$$(x, y) = \frac{1}{4} \Big( \norm{x + y}^{2} - \norm{x - y}^{2} +i \norm{x + iy}^{2} -i \norm{x - iy}^{2} \Big)$$
\end{theorem}

\begin{proof}
For the real case, 
$$ 
\begin{aligned}
\norm{x + y}^{2} - \norm{x - y}^{2} &= (x + y, x + y) - (x - y, x - y) \\
&= \norm{x}^{2} + \norm{y}^{2} + 2 (x, y) - \norm{x}^{2} - \norm{y}^{2} + 2 (x, y) \\ 
&= 4 (x, y)
\end{aligned}
$$

For the complex case, 
$$
\begin{aligned}
\sum_{k = 0}^{3} i^{k} \norm{x + i^{k}y}^{2} &= \sum_{k = 0}^{3} i^{k} (x + i^{k}y, x + i^{k}y) \\
&= \sum_{k = 0}^{3} i^{k} \Big( \norm{x}^{2} + \norm{y}^{2} + (x, i^{k}y) + (i^{k}y, x) \Big) \\
&= \sum_{k = 0}^{3} \Big( i^{k} \norm{x}^{2} + i^{k} \norm{y}^{2} + (x, y) + (i^{2k} y, x) \Big) \\
&= 4 (x, y)
\end{aligned}
$$
where the last step follows from 
$$ \sum_{k = 0}^{3} i^{k} = \sum_{k = 0}^{3} i^{2k} = 0$$
\end{proof}

\begin{theorem}[Parallelogram Identity]
Another important property of the norm is the parallelogram identity. For vectors $u$ and $v$: 
$$\norm{u + v}^{2} + \norm{u - v}^{2} = 2( \norm{u}^{2} + \norm{v}^{2} )$$
\end{theorem}

\begin{proof}
The theorem follows easily from the fact that the sum of the diagonals of a parallelogram equal the sum of all four sides. 
\end{proof}

To review, we have just proved the following properties about the norm $\norm{u}$: 
\begin{enumerate}
	\item Homogeneity: $\norm{\alpha u} = \abs{\alpha} \cdot \norm{u}$ 
	\item Triangle inequality: $\norm{u + v} \leq \norm{u} + \norm{v}$
	\item Non-negativity: $\norm{u} \geq 0$ 
	\item Non-degeneracy: $\norm{u} = 0$ if and only if $u = 0$
\end{enumerate}

In a vector space $V$, if we assign to each vector $u$ a number $\norm{u}$ that satisfies these 4 properties, we can say that the space $V$ is a \textbf{normed space}. 

\section{Orthogonality} 
\begin{definition}
Two vectors $u$ and $v$ are \textbf{orthogonal}, denoted $u \perp v$, if and only if $(u, v) = 0$
\end{definition}

\begin{theorem}
If $u \perp v$, then 
$$\norm{u + v}^{2} = \norm{u}^{2} + \norm{v}^{2}$$
\end{theorem}

\begin{proof}
$$\norm{u + v}^{2} = \norm{u}^{2} + \norm{v}^{2} + (u, v) + (v, u) = \norm{u}^{2} + \norm{v}^{2}$$
since $(u,v) = (v, u) = 0$ by definition of orthogonality. 
\end{proof}

\begin{definition}
A vector $u$ is \textbf{orthogonal to vector space} $V$ if $u$ is orthogonal to all vectors in $V$. 
\end{definition}

\begin{theorem}
Let $V$ be spanned by $v_{1}, \cdots, v_{n}$. Then $u \perp V$ if and only if 
$$u \perp v_{k} \qquad \forall k = 1, \cdots, n$$
\end{theorem}

\begin{proof}
Proving ``only if'' is trivial by the definition of $u \perp V$. Proving ``if'' comes easily after noticing that any vector can be rewritten as a linear combination of the basis vectors, so if $u$ is perpendicular to all the basis vectors, then it is perpendicular to any other vector in $V$. 
\end{proof}

\begin{definition}
A set of vectors $v_{1}, \cdots, v_{n}$ are orthogonal if any two vectors in the set are orthogonal to each other. If $\norm{v_{k}} = 1$ for all $k$, we call the set orthonormal. 
\end{definition}

\begin{lemma}[Generalized Pythagorean Theorem]
Let $v_{1}, \cdots, v_{n}$ be an orthogonal system. Then 
$$\norm{\sum_{k=1}^{n} a_{k} v_{k}}^{2} = \sum_{k=1}^{n} \abs{a_{k}}^{2} \norm{v_{k}}^{2}$$
\end{lemma}

\begin{lproof}
$$\norm{\sum_{k=1}^{n} a_{k} v_{k}}^{2} = \Big( \sum_{k=1}^{n} a_{k} v_{k}, \sum_{j=1}^{n} a_{j} v_{j} \Big) = \sum_{k=1}^{n} \sum_{j=1}^{n} a_{k} \overline{a_{j}} (v_{k}, v_{j})$$

Since the set is orthogonal, $(v_{k}, v_{j})$ is only nonzero when $k = j$, so 
$$= \sum_{k=1}^{n} \abs{a_{k}}^{2} \norm{v_{k}}^{2}$$
\end{lproof}

\begin{definition}
An orthogonal set of vectors that is also a basis is called an \textbf{orthogonal basis}. 
\end{definition}

Typically, to find coordinates of a vector in a basis, we need to solve a system of equations. For orthogonal bases, it is much simpler. Suppose $v_{1}, \cdots, v_{n}$ is an orthogonal basis and let 
$$x = \alpha_{1} v_{1} + \cdots + \alpha_{n} v_{n}$$

Taking the inner product with $v_{1}$ yields 
$$(x, v_{1}) = (\sum_{j=1}^{n} \alpha_{j} (v_{j}, v_{1}) = \alpha_{1} (v_{1}, v_{1}) = \alpha_{1} \norm{v_{1}}^{2}$$

Thus, to find any coordinate $\alpha_{k}$ of a vector $x$ in orthogonal basis $v_{1}, \cdots, v_{n}$: 
$$ \alpha_{k} = \frac{(x, v_{k})}{\norm{v_{k}}^{2}}$$

This is a simple example of abstract orthogonal Fourier decomposition -- simple because classical Fourier decomposition deals with infinite orthonormal systems. 

\section{Orthogonal Projection and Gram-Schmidt Orthogonalization} 

\begin{definition}
The \textbf{orthogonal projection} of a vector $v$ onto the subspace $E$ is the vector $w := P_{E} v$ such that $w \in E$ and $v - w \perp E$. 
\end{definition}

\begin{theorem}
The orthogonal projection $w = P_{E} v$ minimizes the distance from $v$ to $E$. In other words, 
$$\norm{v - w} \leq \norm{v - x} \qquad \forall x \in E$$
Additionally, if for some $x \in E$
$$\norm{v - w} = \norm{v - x}$$
then $x = w$. 
\end{theorem}

\begin{proof}
Let $y = w - x \in E$. Then 
$$v - x = v - w + w - x = v - w + y$$
Since $v - w \perp E$, we know $v - w \perp y$. By the Pythagorean Theorem, 
$$\norm{v - x}^{2} = \norm{v - w}^{2} + \norm{y}^{2} \geq \norm{v - w}^{2}$$

To finish the proof, note that equality only arises when $y = 0$, ie when $x = w$. 
\end{proof}

There is a formula for finding an orthogonal projection if we know an orthogonal basis in $E$. Let $v_{1}, \cdots, v_{n}$ be an orthogonal basis in $E$. Then the projection $P_{E} v$ of a vector $v$ is 
$$P_{E} v = \sum_{k=1}^{n} a_{k} v_{k} \qquad where \qquad a_{k} = \frac{(v, v_{k})}{\norm{v_{k}}^{2}}$$

In other words, 
$$P_{E} v = \sum_{k=1}^{n} \frac{(v, v_{k})}{\norm{v_{k}}^{2}} v_{k}$$

Note that this only works if we have an \textit{orthogonal} basis. However, even if we only have a basis in $E$, we can use the following algorithm to find an orthogonal basis. 

\begin{theorem}[Gram-Schmidt Orthogonalization Algorithm]
Suppose we have linearly independent system $x_{1}, \cdots, x_{n}$. The Gram-Schmidt algorithm constructs from this an orthogonal system $v_{1}, \cdots, v_{n}$ such that 
$$span(x_{1}, \cdots, x_{n}) = span(v_{1}, \cdots, v_{n})$$
Additionally, for all $r \leq n$ 
$$span(x_{1}, \cdots, x_{r}) = span(v_{1}, \cdots, v_{r})$$

The algorithm is as follows: 
\begin{enumerate}
	\item Define $v_{1} := x_{1}$. 
	
	Define $E_{1} := span(v_{1}) = span(x_{1})$. 
	\item Define $v_{2} := x_{2} - P_{E_{1}} x_{2} = x_{2} - \frac{(x_{2}, v_{1})}{\norm{v_{1}}^{2}} v_{1}$. 
	
	Define $E_{2} := span(v_{1}, v_{2}) = span(x_{1}, x_{2})$. 
	\item Define $v_{3} := x_{3} - P_{E_{2}} x_{3} = x_{3} - \frac{(x_{3}, v_{1})}{\norm{v_{1}}^{2}} v_{1} - \frac{(x_{3}, v_{2})}{\norm{v_{2}}^{2}} v_{2}$. 
	
	Define $E_{3} := span(v_{1}, v_{2}, v_{3}) = span(x_{1}, x_{2}, x_{3})$. 
	
	\item Continue until we have $n$ vectors and $span(v_{1}, \cdots, v_{n}) = span(x_{1}, \cdots, x_{n})$. The formula for vector $v_{r + 1}$ given $v_{1}, \cdots, v_{r}$ is 
	$$ v_{r + 1} := x_{r + 1} - P_{E_{r}} x_{r + 1} = x_{r + 1} - \sum_{k = 1}^{r} \frac{(x_{r + 1}, v_{k})}{\norm{v_{k}}^{2}} v_{k}$$
\end{enumerate}

Note that at each step, we are adding in $x_{r + 1}$ which means the resulting vector will not exist in $E_{r}$. 
\end{theorem}

\begin{proof}
At each step, we add in $x_{r + 1}$ and then subtract its projection the subspace spanned by $x_{1}, \cdots, x_{r}$, meaning each additional vector is orthogonal to the ones previously defined. Since we set $v_{1} = x_{1}$, we have proved the algorithm by induction. 
\end{proof}

Since multiplication by a scalar does not change orthogonality, we can multiply vectors $v_{k}$ returned by Gram-Schmidt by any non-zero numbers. One use case is to normalize the orthogonal vectors by dividing by their norms $\norm{v_{k}}$ to yield an orthonormal system. 


\begin{theorem}
A projector $P$ is independent of the basis used to define the subspace $W$ being projected onto. 
\end{theorem}

\begin{proof}
Suppose we are projecting $v \in V$ onto $W$. Define $w_{1}$ and $w_{2}$ as two projections of $v$ onto $W$. 

By definition, $(v - w_{1}) \perp W$ and $(v - w_{2}) \perp W$. 
$$
\begin{aligned}
(v - w_{2} - (v - w_{1})) \perp W &\rightarrow (w_{1} - w_{2}) \perp W \\
&\rightarrow (w_{1} - w_{2}) \perp (w_{1} - w_{2}) \\
&\rightarrow w_{1} - w_{2} = 0
\end{aligned}
$$
where the second step follows from $(w_{1} - w_{2}) \in W$ and the third step follows from the fact that only the 0 vector is orthogonal to itself. This implies both projections are the same. Since this is true for all projections $P_{W} v$, $P$ is unique. 
\end{proof}

The following is an important, defining property of projections. 

\begin{theorem}
For any orthogonal projection, $P: V \rightarrow E$, $$P^{2} = P$$
\end{theorem}

\begin{proof}
We know that for some orthogonal basis $v_{1}, \cdots, v_{n}$ for $E$, 
$$P_{E} v = \sum_{k=1}^{n} \frac{(v, v_{k})}{\norm{v_{k}}^{2}} v_{k}$$

If $v \in E$, then 
$$v = \sum_{k=1}^{n} \frac{(v, v_{k})}{\norm{v_{k}}^{2}} v_{k}$$
so $P_{E} v = v$ for all $v \in E$. 

Intuitively, projecting $v \in E$ onto $E$ would clearly result in $v$. 
\end{proof}

The idea of orthogonality is not unique to vector spaces. As an example, we will use Gram-Schmidt to construct a set of \textit{orthogonal polynomials} over the interval [-1,1]. Let us define the inner product on the space of polynomials as $(f, g) = \int_{-1}^{1} f(t) g(t) dt$. Now, we will use Gram-Schmidt to construct an orthogonal basis of polynomials from the basis $1, t, t^{2}, t^{3}$ such that for each polynomial $P(1) = 1$.

We know that 
$$v_{1} := 1$$
Then, we define 
$$v_{2} := t - \frac{(t, 1)}{(1, 1)} 1 = t - \frac{\int_{-1}^{1} t dt}{\int_{-1}^{1} dt} = t
$$

Next, we define 
$$v_{3} := t^{2} - \frac{(t^{2}, 1)}{(1, 1)} 1 - \frac{(t^{2}, t)}{(t, t} t = t^{2} - \frac{\int_{-1}^{1} t^{2} dt}{\int_{-1}^{1} dt} - \frac{\int_{-1}^{1} t^{3} dt}{\int_{-1}^{1} t^{2} dt} = \frac{1}{2} (3t^{2} - 1)$$

Note that we scaled $v_{3}$ to satisfy $P(1) = 1$. Finally, we define 
$$v_{4} := t^{3} - \frac{(t^{3}, 1)}{(1, 1)} 1 - \frac{(t^{3}, t)}{(t, t)} t - \frac{(t^{3}, t^{2})}{(t^{2}, t^{2})} t^{2} = t^{3} - \frac{\int_{-1}^{1} t^{4} dt}{\int_{-1}^{1} t^{2} dt} t = \frac{1}{2} (5t^{3} - 3t)$$

These are the first 4 of \textbf{Legendre's polynomials}, a set of important polynomials which appear in many different branches of mathematics. 

\subsection{Orthogonal Complement}
\begin{definition}
For a subspace $E$, its \textbf{orthogonal complement $E^{\perp}$} is the set of all vectors orthogonal to $E$. Since at least 0 is orthogonal to $E$, $E^{\perp}$ is always a subspace. 
\end{definition}

By the definition of orthogonal projection, any vector in an inner product space $V$ has a unique representation of the form 
$$v = v_{1} + v_{2} \qquad v_{1} \in E, v_{2} \in E^{\perp}$$
This statement is usually written as $V = E \oplus E^{\perp}$.

\begin{theorem}
For subspace $E$ of $V$, 
$$(E^{\perp})^{\perp} = E$$
\end{theorem}

\begin{proof}
We will show $E \subseteq (E^{\perp})^{\perp}$ and $(E^{\perp})^{\perp} \subseteq E$.

Let $u \in E$. Then $(u, v) = 0$ for all $v \in E^{\perp}$. Since $u$ is orthogonal to every vector $v \in E^{\perp}$, then $u \in (E^{\perp})^{\perp}$ so $E \subseteq (E^{\perp})^{\perp}$. 

Now let $u \in (E^{\perp})^{\perp}$. Since $V = E \oplus E^{\perp}$, we can write $u = v + w$, where $v \in E$ and $w \in E^{\perp}$. This means that $u - v = w \in E^{\perp}$. Since we know $E \subseteq (E^{\perp})^{\perp}$, we have $u \in  (E^{\perp})^{\perp}$ and $v \in (E^{\perp})^{\perp}$, which means $u - v \in  (E^{\perp})^{\perp}$. Therefore, $u - v \in E^{\perp} \cap (E^{\perp})^{\perp}$. Since the only vector that is orthogonal to itself is 0, $u = v$, and because $v \in E$, $(E^{\perp})^{\perp} \subseteq E$. 
\end{proof}

Now suppose $P$ is the orthogonal projection onto subspace $E$, and $Q$ is the orthogonal projection onto the orthogonal complement $E^{\perp}$. 

\begin{lemma}
$$ P + Q = I \text{ and } PQ = 0$$ 
\end{lemma}

\begin{lproof}
For any inner product space $V$, we have $V = E \oplus E^{\perp}$. This means that for any vector $v \in V$, $P(v) = e_{1}$ where $e_{1} \in E$, and so $(I - P)v = v - e_{1} = e_{2}$ where $e_{2} \in E^{\perp}$, which means $Q = I - P$. 

Now, $$P + Q = P + I - P = I$$
and $$PQ = P(I - P) = P - P^{2} = P - P = 0$$
\end{lproof}

\begin{theorem}
$P - Q$ is its own inverse. 
\end{theorem}

\begin{proof}
We must show that $(P-Q)(P-Q) = I$. 

Expanding
$$(P-Q)(P-Q) = P^{2} - PQ - QP + Q^{2} = P + Q = I$$
\end{proof}


\section{Least Square Solution} 
Recall that $Ax = b$ has a solution if and only if $b \in Range(A)$. In real life, it is impossible to avoid errors. The simplest way to approximate a solution is to choose an approximation $\hat{x}$ to minimize the error $e = \norm{A \hat{x} - b}$. This is the \textbf{least square solution}. 

We know $A \hat{x}$ is the orthogonal projection $P_{Range(A)} b$ if and only if $b - A \hat{x} \perp Range(A)$. Using the column space interpretation of range, this is equivalent to 
$$b - A \hat{x} \perp a_{k} \qquad \forall k = 1, \cdots, n$$
That means 
$$0 = (b - A \hat{x}, a_{k}) = a^{*}_{k} (b - A \hat{x}) \qquad \forall k = 1, \cdots, n$$ 
We can join the rows $a^{*}_{k}$ together to get
$$A^{*} (b - A \hat{x}) = 0$$
which is equivalent to the \textbf{normal equation} 
$$A^{*} A \hat{x} = A^{*} b$$

The solution $\hat{x}$ to this equation grants us the least square solution of $A \hat{x} = b$. This makes it easy to notice that the least square solution is unique if and only if $A^{*} A$ is invertible. 

If $\hat{x}$ is the solution to the normal equation, then $A \hat{x} = P_{Range(A)} b$. So in order to find the actual projection of $b$ onto $Range(A)$, we need to solve the normal equation and then multiply the solution by $A$. Formally, 
$$P_{Range(A)} b = A (A^{*} A)^{-1} A^{*} b$$
Because this is true for all $b$, the formula for the matrix of the orthogonal projection onto $Range(A)$ is 
$$P_{Range(A)} = A (A^{*} A)^{-1} A^{*}$$

\begin{theorem}
For an $m \times n$ matrix $A$ 
$$Ker(A) = Ker(A^{*}A)$$

Recall Kernel is equivalent to Null Space.
\end{theorem}

\begin{proof}
We will show $Ker(A) \subseteq Ker(A^{*} A)$ and $Ker(A^{*} A) \subseteq Ker(A)$. 

To prove the latter, suppose we have a vector $u \in Ker(A)$ so that $Au = 0$. Then $A^{*} A u = A^{*} (Au) = A^{*} 0 = 0$, which means $u \in Ker(A^{*} A)$. 

To prove the former, suppose we have a vector $v \in Ker(A^{*} A)$. We want to show that $Av = 0$. One way of doing so is to show that its norm is 0. 
$$\norm{Av}^{2} = (Av, Av) = (A^{*} v^{*}, A^{*} v^{*}) = A^{*} (v^{*}, A^{*} v^{*}) = A^{*} (Av, v) = (A^{*} Av, v) = (0, v) = 0$$
\end{proof}

\section{Adjoint of a Linear Transformation} 
Recall that the \textit{Hermitian adjoint} $A^{*}$ of matrix $A$ is defined as the complex conjugate of each entry in $A^{T}$. 

\begin{theorem}
$$(Ax, y) = (x, A^{*}y) \qquad \forall x \in \C^{n}, \forall y \in \C^{m}$$
\end{theorem}

\begin{proof}
$$(Ax, y) = y^{*} Ax = (A^{*} y)^{*} x = (x, A^{*} y)$$

The second equality uses the fact that because the adjoint consists of a transpose, we have $(AB)^{*} = B^{*} A^{*}$ and $(A^{*})^{*} = A$.  
\end{proof}

This identity is used to define the adjoint operator. 

\begin{lemma}
The adjoint is unique. 
\end{lemma}

\begin{lproof}
Suppose $B$ satisfies $(Ax, y) = (x, By) \qquad \forall x, y$, then we can write 
$$(Ax, y) = (x, A^{*} y) = (x, By)$$
which means $A^{*} = B$.
\end{lproof}

Properties of the adjoint operator:
\begin{enumerate}
	\item $(A + B)^{*} = A^{*} + B^{*}$ 
	\item $(\alpha A)^{*} = \overline{\alpha} A^{*}$ 
	\item $(AB)^{*} = B^{*} A^{*}$ 
	\item $(A^{*})^{*} = A$
	\item $(y, Ax) = (A^{*}y, x)$ 
\end{enumerate}

\begin{theorem}[Relation between fundamental subspaces]
Let $A: V \rightarrow W$ be an operator acting from one inner product space to another. Then 
\begin{enumerate}
	\item $Ker(A^{*}) = (Range(A))^{\perp}$ 
	\item $Ker(A) = (Range(A^{*}))^{\perp}$ 
	\item $Range(A) = (Ker(A^{*}))^{\perp}$ 
	\item $Range(A^{*}) = (Ker(A))^{\perp}$
\end{enumerate}

Note that earlier we defined the fundamental subspaces using $A^{T}$ instead of $A^{*}$ because when discussing only $\R$ there was no difference. 
\end{theorem}

\begin{proof}
Note that statements 1/3 and 2/4 are equivalent because for any subspace $E$, we have $(E^{\perp})^{\perp} = E$. Also note that statement 2 is exactly statement 1 applied to the operator $A^{*}$ since $(A^{*})^{*} = A$. 

Thus we only need to prove statement 1. 

A vector $x \in (Range(A))^{\perp}$ means that $x$ is orthogonal to all vectors of the form $Ay$, that is 
$$(x, Ay) = 0 \qquad \forall y$$ 
Since $(x, Ay) = (A^{*}x, y)$, this is equivalent to 
$$(A^{*} x, y) = 0 \qquad \forall y$$

This means that $A^{*}x = 0$, which means $x \in Ker(A^{*})$. 
\end{proof}

We can use the idea of adjoint operators to prove another property of orthogonal projections. 

\begin{theorem}
If $P: V \rightarrow E$ is an orthogonal projection, it is self-adjoint. 
\end{theorem}

\begin{proof}
Recall that any $v_{k}$ can be written as a sum of vectors in $E$ and $E^{\perp}$, $v_{k} = e_{k} + \widetilde{e_{k}}$. Then
$$(Pv_{1}, v_{2}) = (u_{1}, u_{2} + \widetilde{u_{2}}) = (u_{1}, u_{2}) + (u_{1}, \widetilde{u_{2}}) = (u_{1}, u_{2})$$
and 
$$(v_{1}, Pv_{2}) = (u_{1} + \widetilde{u_{1}}, u_{2}) = (u_{1}, u_{2}) + (\widetilde{u_{1}}, u_{2}) = (u_{1}, u_{2})$$
Since $(Tv_{1}, v_{2}) = (v_{1}, Tv_{2})$, $T = T^{*}$. 
\end{proof}



\section{Isometries and Unitary Operators}
\begin{definition}
An operator $U: X \rightarrow Y$ is called an \textbf{isometry} if it preserves the norm, 
$$\norm{Ux} = \norm{x} \qquad \forall x \in X$$
\end{definition}

\begin{theorem}
An operator $U: X \rightarrow Y$ is an isometry if and only if it preserves the inner product, ie if and only if 
$$(x, y) = (Ux, Uy) \qquad \forall x, y \in X$$
\end{theorem}

\begin{proof}
We use the polarization identities previously described. If $X$ is a complex space

$$
\begin{aligned}
(Ux, Uy) &= \frac{1}{4} \sum_{\alpha = \pm 1, \pm i} \alpha \norm{Ux + \alpha Uy}^{2} \\ 
&= \frac{1}{4} \sum_{\alpha = \pm 1, \pm i} \alpha \norm{U(x + \alpha y)}^{2} \\
&= \frac{1}{4} \sum_{\alpha = \pm 1, \pm i} \alpha \norm{x + \alpha y}^{2} = (x, y)
\end{aligned}
$$

If $X$ is a real space 
$$\begin{aligned} 
(Ux, Uy) &= \frac{1}{4}(\norm{Ux + Uy}^{2} - \norm{Ux - Uy}^{2}) \\
&= \frac{1}{4} (\norm{U(x + y)}^{2} - \norm{U(x - y)}^{2}) \\
&= \frac{1}{4} (\norm{x + y}^{2} - \norm{x - y}^{2}) = (x, y)
\end{aligned}
$$
\end{proof}

\begin{lemma}
An operator $U: X \rightarrow Y$ is an isometry if and only if $U^{*}U = I$. 
\end{lemma}

\begin{lproof}
If $U^{*}U = I$, then 
$$(x, x) = (U^{*}Ux, x) = (Ux, Ux) \qquad \forall x \in X$$
Since $\norm{x} = \norm{Ux}$, $U$ is an isometry. 

If $U$ is an isometry, then by the above theorem and definition of adjoint
$$(U^{*} Ux, y) = (Ux, Uy) = (x, y) \qquad \forall x, y \in X$$
which means $U^{*}U = I$. 
\end{lproof}

This lemma implies that an isometry is always left invertible since $U^{*}U = I$. 

\begin{definition}
An isometry $U: X \rightarrow Y$ is called a \textbf{unitary operator} if it is invertible.  
\end{definition}

\begin{lemma}
An isometry $U: X \rightarrow Y$ is a unitary operator if and only if $dim(X) = dim(Y)$. 
\end{lemma}

\begin{lproof}
If $dim(X) = dim(Y)$, then $U$ is square. Since we know $U$ is left invertible, it must also then be invertible. 

If $U$ is unitary, it is invertible, so $dim(X) = dim(Y)$ since only square matrices are invertible. 
\end{lproof}

\begin{theorem}
The product of unitary operators is also unitary. 
\end{theorem}

\begin{proof}
Let $A$ and $B$ be unitary operators. We must show that $(AB)^{*} (AB) = I$. 

$$(AB)^{*}(AB) = B^{*} A^{*} A B = B^{*} B = I$$
\end{proof}

Properties of unitary operators that follow from our proofs: 
\begin{enumerate}
	\item $U^{-1} = U^{*}$ 
	\item If $U$ is unitary, $U^{*} = U^{-1}$ is also unitary. 
	\item If $U$ is an isometry and $v_{1}, \cdots, v_{n}$ is an orthonormal basis, then $Uv_{1}, \cdots, Uv_{n}$ is an orthonormal basis. 
	\item The product of unitary operators is a unitary operator. 
\end{enumerate}

\begin{lemma}
$$det(A^{*}) = \overline{det(A)}$$
\end{lemma}

\begin{lproof}
Recall that the determinant of a matrix is equal to the product of its eigenvalues. We will show that for any eigenvalue $\lambda$ of $A$, $\overline{\lambda}$ is an eigenvalue of $A^{*}$. 

Note that $\lambda$ is \textbf{not} an eigenvalue of $A$ if and only if $A - \lambda I$ is invertible, which happens if and only if there exists an operator $B$ such that 
$$B(A - \lambda I) = (A - \lambda I)B = I$$
Taking the adjoints of all three sides means the above is equivalent to 
$$(A^{*} - \overline{\lambda}I)B^{*} = B^{*} (A^{*} - \overline{\lambda}I) = I$$

Thus $A - \lambda I$ is invertible if and only if $A^{*} - \overline{\lambda}I$ is invertible, which means if $\lambda$ is an eigenvalue of $A$, $\overline{\lambda}$ is an eigenvalue of $A^{*}$. 
\end{lproof}

\begin{theorem}
If $U$ is a unitary matrix, then 
$$ det(U) = \pm 1$$
If $\lambda$ is an eigenvalue of $U$, then 
$$\lambda = e^{i \theta}$$
for some real $\theta$. 
\end{theorem}

\begin{proof}
Let $det(U) = z$. Since $det(U^{*}) = \overline{det(U)}$, we have 
$$\abs{z}^{2} = \overline{z}z = det(U^{*} U) = det(I) = 1$$

To prove statement 2, notice that the condition $\lambda = e^{i \theta}$ requires we prove $\abs{\lambda} = 1$. 

Now, if $Ux = \lambda x$, then 
$$\norm{Ux} = \norm{\lambda x} = \abs{\lambda} \cdot \norm{x}$$
which means $\abs{\lambda} = 1$ since $\norm{Ux} = \norm{x}$. 
\end{proof}

\begin{definition}
Operators $A$ and $B$ are called \textbf{unitarily equivalent} if there exists a unitary operator $U$ such that $A = UBU^{*}$. Since for any unitary $U$, we have $U^{-1} = U^{*}$, any two unitarily equivalent matrices are similar as well. 

The converse is \textbf{not} true. 
\end{definition}

The following theorem gives a way to construct a counter example to prove similar matrices are not always unitarily equivalent. 

\begin{theorem}
A matrix $A$ is unitarily equivalent to a diagonal one if and only if it has an orthogonal (orthonormal) basis of eigenvectors. 
\end{theorem}

\begin{proof}
Using diagonalization, we can write $A = UBU^{*}$ and let $Bx = \lambda x$. Then $AUx = UBx = \lambda Ux$, which means $Ux$ is an eigenvector of $A$. 

Only if: Let $A$ be unitarily equivalent to a diagonal matrix $D$, ie $A = UDU^{*}$. Because $D$ is diagonal, the vectors $e_{k}$ of the standard basis are eigenvectors of $D$, so $Ue_{k}$ are eigenvectors of $A$. Since $U$ is unitary, $Ue_{1}, \cdots, Ue_{n}$ is an orthonormal basis. 

If: Let $A$ have an orthogonal basis $u_{1}, \cdots, u_{n}$ of eigenvectors. By dividing each vector by its norm, we can assure we have an orthonormal basis. By letting $D$ be the matrix $A$ in the basis $u_{1}, \cdots, u_{n}$, we know $D$ will be a diagonal matrix. 

By setting $U$ to be the matrix with columns $u_{1}, \cdots, u_{n}$, we know $U$ is unitary since its columns form an orthonormal basis (orthogonality implies invertibility and normality implies norm preservation). The change of coordinate formula implies 
$$A = [A]_{SS} = [I]_{SB} [A]_{BB} [I]_{BS} = UDU^{-1} = UDU^{*}$$
where the last step follows from $U^{-1} = U^{*}$ for unitary matrices. 
\end{proof}

%%% DID NOT DO RIGID MOTIONS AND COMPLEXIFICATION!!
%%% DID NOT DO RIGID MOTIONS AND COMPLEXIFICATION!!
%%% DID NOT DO RIGID MOTIONS AND COMPLEXIFICATION!!