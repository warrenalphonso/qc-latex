\chapter{Spectral Theory}
Spectral theory will be our main tool for analyzing linear operators. In this chapter, we only consider transformations $A: V \rightarrow V$ ($n \times n$ matrices). 

\section{Definitions}

\begin{definition}
A scalar $\lambda$ is called an \textbf{eigenvalue} of operator $A: V \rightarrow V$ if there exists a \textit{nonzero} vector $v \in V$ such that 
$$Av = \lambda v$$

The vector $v$ is called an \textbf{eigenvector} of $A$ (corresponding to the eigenvalue $\lambda$).

Once we know the eigenvalues, finding the eigenvectors is equivalent to solving 
$$(A - \lambda I)v = 0$$
Additionally, $\Null(A - \lambda I)$, the set of all eigenvectors and 0, is called the \textbf{eigenspace}. 

The set of all eigenvalues of an operator is called the \textbf{spectrum} of $A$, denoted $\sigma (A)$. 
\end{definition}

Since the matrix $A$ is square, $A - \lambda I$ has a nontrivial null space if and only if it is not invertible. Thus,
$$\Det(A - \lambda I) = 0$$

\begin{definition}
If $A$ is an $n \times n$ matrix, $\Det(A - \lambda I)$ is a degree-$n$ polynomial of variable $\lambda$. This is called the \textbf{characteristic polynomial} of $A$. Finding the spectrum of $A$ requires finding the roots to the characteristic polynomial. 

Using $\Det(\lambda I - A)$ as the characteristic polynomial always yields a monic polynomial, whereas our current definition differs by a factor of $(-1)^{n}$. This makes no difference for properties like having eigenvalues located at roots so the two definitions are usually interchangeable. 

This means any operator in $\mathbb{C}^{n}$ has $n$ eigenvalues, though some may be repeated. 
\end{definition}

\begin{theorem}
An $n \times n$ matrix $A$ is invertible if and only if it doesn't have an eigenvalue of 0. 
\end{theorem}

\begin{proof}
Proving if: 

If $A$ doesn't have an eigenvalue of 0, then $\Det(A - 0I) \neq 0 \rightarrow \Det(A) \neq 0$, which implies $A$ is invertible. 

Proving only if: 

If $A$ is invertible, then $\Det(A) \neq 0$, which implies $\Det(A - 0I) \neq 0$. 
\end{proof}

\begin{theorem}
Let $A$ be an $n \times n$ matrix, and let $\lambda_{1}, \cdots, \lambda_{n}$ be its complex eigenvalues (counting multiplicities). Then
$$\Det(A) = \lambda_{1} \cdots \lambda_{n}$$
\end{theorem}


\begin{proof}
Since the characteristic polynomial is degree-$n$ with variable $\lambda$ and $A$ has $n$ eigenvalues, we can write 

$$\Det(\lambda I - A) = (\lambda - \lambda_{1}) \cdots (\lambda - \lambda_{n})$$

Plugging in $\lambda = 0$ gives us 
$$\Det(-A) = (-1)^{n} \Det(A) = (- \lambda_{1}) \cdots (- \lambda_{n})$$

which simplifies to 
$$\Det(A) = (\lambda_{1}) \cdots (\lambda_{n})$$
\end{proof}


\begin{theorem}
Let $A$ be an $n \times n$ matrix, and let $\lambda_{1}, \cdots, \lambda_{n}$ be its complex eigenvalues (counting multiplicities). Then
$$\Trace(A) = \lambda_{1} + \cdots + \lambda_{n}$$ 
\end{theorem}

\begin{proof}
Recall that $A_{jk}$ is the entry of $A$ in row $j$ and column $k$ while $A_{j, k}$ is the cofactor without row $j$ and column $k$. 

We begin by analyzing $\Det(A - \lambda I)$. If we use cofactor expansion across any row, any term involving an off-diagonal entry ($A_{jk} : j \neq k$) results in a determinant with degree $n - 2$. This is because cofactoring removes the row and column the chosen entry is on, and since $j \neq k$, we remove the variables $A_{jj} - \lambda$ and $A_{kk} - \lambda$. This means the $\lambda^{n}$ and $\lambda^{n-1}$ terms come from the product of the diagonal entries in $\Det(A - \lambda I)$. In other  words, 
$$\Det(A - \lambda I) = (A_{11} - \lambda) \cdots (A_{nn} - \lambda) + q(\lambda)$$
where $q(\lambda)$ is a polynomial with degree $n - 2$. 

To find the coefficient of the $\lambda^{n-1}$ term, we simply 
$$ (A_{11} - \lambda) \cdots (A_{nn} - \lambda) = (-1)^{n} (\lambda - A_{11}) \cdots (\lambda - A_{nn})$$
and notice that the $\lambda^{n-1}$ term is formed by choosing $\lambda$ $n - 1$ times and then choosing $A_{kk}$ to get 
$$ (-1)^{n} (A_{11} \lambda^{n-1}) \cdots (A_{nn} \lambda^{n-1}) = (-1)^{n} (A_{11} + \cdots + A_{nn}) \lambda^{n-1}$$

Now we will rewrite the coefficient of the $\lambda^{n-1}$ term using the characteristic equation. 
$$\Det(A - \lambda I) = (\lambda_{1} - \lambda) \cdots (\lambda_{n} - \lambda) = (-1)^{n} (\lambda - \lambda_{1}) \cdots (\lambda - \lambda_{n})$$

Again, we have to choose the $\lambda$ term $n - 1$ times, so we get 
$$(-1)^{n} (\lambda_{1} \lambda^{n-1}) \cdots (\lambda_{n} \lambda^{n-1}) = (-1)^{n} (\lambda_{1} + \cdots + \lambda_{n})\lambda^{n-1}$$

Comparing coefficients yields $\Trace(A) = \lambda_{1} + \cdots + \lambda_{n}$. 
\end{proof}

\section{Diagonalization}
We can use spectral theory to find the diagonalization of operators, which means that given an operator, we find the basis in which the matrix of the operator is diagonal. This makes powers of an operator much easier to compute. 

\begin{theorem}
A matrix $A$ in $\mathbb{F}^{n}$ can be written as $A = PDP^{-1}$, where $D$ is a diagonal matrix and $P$ is invertible, if and only if there exists a basis in $\mathbb{F}^{n}$ of eigenvectors of $A$. 

In this case, the diagonal entries of $D$ are the eigenvalues of $A$ and the columns of $P$ are the corresponding eigenvectors. 
\end{theorem}

\begin{proof}

To understand the intuition behind this, note that $P = [I]_{SB}$, where $S$ is the standard basis and $B$ is the basis for the eigenspace, since each column is the representation of a basis vector written in $S$. Rewriting $A = PDP^{-1}$ as $D = P^{-1}AP = [I]_{BS} A [I]_{SB}$ which means $D = [A]_{BB}$, which is a diagonal operator if and only if its diagonal entries are eigenvalues whose corresponding eigenvectors are $b_{k}$. Think of the operator $[I]_{BS} A [I]_{SB}$ as converting a vector to a basis of eigenvectors, scaling those eigenvectors appropriately by their eigenvalues, and then converting back to the standard basis. 

A simpler, more direct proof is to rewrite $AP = PD$. 
$$AP = \begin{bmatrix}
Ab_{1} & \cdots & Ab_{n}
\end{bmatrix} = \begin{bmatrix}
\lambda_{1} b_{1} & \cdots & \lambda_{n} b_{n}
\end{bmatrix}$$

$$PD = \begin{bmatrix}
b_{1} \lambda_{1} & \cdots & b_{1} \lambda_{1}
\end{bmatrix} = \begin{bmatrix}
\lambda_{1} b_{1} & \cdots & \lambda_{n} b_{n}
\end{bmatrix}$$
\end{proof}

Of course, for $P$ to be invertible, the eigenvectors $b_{1}, \cdots, b_{n}$ must be linearly independent. Luckily, we can easily check if this is the case with the following theorem. 

\begin{theorem}
Let $\lambda_{1}, \cdots, \lambda_{n}$ be \textbf{distinct} eigenvalues for $A$, and let $b_{1}, \cdots, b_{n}$ be their corresponding eigenvectors. Then $b_{1}, \cdots, b_{n}$ are linearly independent. 
\end{theorem}

\begin{proof}
We proceed by induction over the $n$ eigenvectors of $A$. 

\textbf{Base case: $n = 1$}

This is trivial because by definition, an eigenvector is nonzero. Any set consisting of a single nonzero vector is linearly independent.

\textbf{Inductive Hypothesis:}

Assume it holds for $n = k$. 

\textbf{Inductive Step: $n = k + 1$}

Suppose there exists a non-trivial solution to 
$$ \sum_{i = 1}^{k + 1} c_{i} b_{i} = 0$$

We can apply $(A - \lambda_{k + 1} I)$ to both sides to get 
$$ \sum_{i = 1}^{k + 1} c_{i} (A - \lambda_{k + 1} I) b_{i} = 0$$

Since $(A - \lambda_{k + 1}I) b_{k + 1} = 0$, we can write 
$$\sum_{i = 1}^{k} c_{i} (A - \lambda_{k + 1} I ) b_{i} = \sum_{i = 1}^{k} c_{i} (\lambda_{i} - \lambda_{k + 1} ) b_{i} = 0$$ 

By the inductive hypothesis, we know the first $k$ eigenvectors are linearly independent, so the coefficient $c_{i} (\lambda_{i} - \lambda_{k + 1} )$ must be 0 for $0 \leq i \leq k$, and since eigenvalues are distinct, $c_{i} = 0$ for $0 \leq i \leq k$. 

Now we can reduce our original summation 
$$\sum_{i = 1}^{k+1} c_{i} b_{i} = c_{k+1} b_{k+1} = 0$$
This means that $c_{k+1}$ must be 0, so the summation only has the trivial solution, which means the eigenvectos are linearly independent. 
\end{proof}