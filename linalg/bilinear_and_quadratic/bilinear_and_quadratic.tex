\chapter{Bilinear and Quadratic Forms - WIP}

\section{Main Definitions}

\begin{definition}
A \textbf{bilinear form} on $\R^{n}$ is a function $L(x, y)$ of arguments $x, y \in \R^{n}$ which is linear in each argument. That is 
\begin{enumerate}
	\item $L(\alpha x_{1} + \beta x_{2}, y) = \alpha L(x_{1}, y) + \beta L(x_{2}, y)$
	\item $L(x, \alpha y_{1} + \beta y_{2}) = \alpha L(x, y_{1}) + \beta L(x, y_{2})$
\end{enumerate}

If $x = \begin{bmatrix}
x_{1} \\
\vdots \\
x_{n}
\end{bmatrix}$ and $y = \begin{bmatrix}
y_{1} \\
\vdots \\
y_{n}
\end{bmatrix}$, a bilinear form can be written as 
$$L(x, y) = \sum_{j, k = 1}^{n} a_{jk} x_{k} y_{j}$$
or in matrix form 
$$L(x, y) = (Ax, y)$$
where the matrix $A$ is uniquely determined by the bilinear form $L$.
\end{definition}

\begin{definition}
There are many definitions of a quadratic form. 

One can define a quadratic form on $\R^{n}$ as the ``diagonal" of a bilinear form $L$, that is, any quadratic form $Q$ is defined as $Q[x] = L(x, x) = (Ax, x)$. 

Another definition is to say a quadratic form is a homogeneous second degree polynomial, that is, $Q$ is a polynomial of $n$ variables $x_{1}, \cdots, x_{n}$ having only terms of degree 2 (only terms $ax_{k}^{2}$ and $cx_{j} x_{k}$ are allowed).

There are infinitely many ways to write a quadratic form as $Q[x] = (Ax, x)$, but if we require $A$ to be \textit{symmetric} ($A^{T} = A$), then $A$ will be unique. 

Any \textbf{quadratic form $Q[x]$} on $\R^{n}$ admits a unique representation $Q[x] = (Ax, x)$ where $A$ is a real symmetric matrix. 

We can extend this definition to $\C^{n}$ by taking the self-adjoint transformation, $A = A^{*}$ and defining $Q[x] = (Ax, x)$. Unless explicitly noted, all our theorems will be true in the complex case as well. 

\end{definition}

The only essential difference is that in the complex case we have no choice: a real quadratic form corresponds to a Hermitian matrix. 

\begin{theorem}
$(Ax, x)$ is real for all $x \in \C^{n}$ if and only if $A = A^{*}$.  
\end{theorem}

\begin{proof}
To prove if: 
$$(Ax, x) = (x, A^{*} x) = (x, Ax) = \overline{(Ax, x)}$$

To prove only if: Consider the expression $(A (x + zy), x + zy)$. We assume it is real for all $z \in \C$. Now we can write 
$$
\begin{aligned}
\Big( A (x + zy), x + zy \Big) &= ( Ax + Azy, x + zy) \\
&= \overline{(x, Ax)} + \overline{(zy, Ax)} + \overline{(x, Azy)} + \overline{(zy, Azy)} \\ 
&= (Ax, x) + \overline{z} (Ax, y) + z (Ay, x) + z \overline{z} (Ay, y)
\end{aligned}
$$

We know that the final sum must be real. Since only the middle two terms can contribute to an imaginary solution, we know 
$$\overline{z} (Ax, y) + z (Ay, x) \in \R$$
Because $z$ can be any complex number, we know $z \neq \overline{z}$, so for the imaginary parts to cancel, we must have
$$(Ax, y) = (Ay, x)$$
and because we know $(Ax, y) = (x, A^{*} y)$, this means that $A^{*} = A$. 
\end{proof}

\section{Diagonalization of Quadratic Forms}

Quadratic forms are common; they appear in the study of curves in $\R^{2}$ and some surfaces in $\R^{3}$, for example. If we are given a set in $\R^{n}$ defined by $Q[x] = 1$, where $Q$ is some quadratic form, we might want to understand the structure of this set. We will try to do so using a change of variables. 

\subsection{Orthogonal Diagonalization}

Suppose we have a quadratic form $Q[x] = (Ax, x)$ in $\F^{n}$. We define $y = S^{-1} x$, where $S$ is any invertible $n \times n$ matrix. Now, we have
$$Q[x] = Q[Sy] = (ASy, Sy) = (S^{*} AS y, y)$$
so when written in variable $y$, the quadratic form has matrix $S^{*}AS$. 

Now our goal is to find an invertible matrix $S$ such that $S^{*} AS$ is diagonal. Using diagonalization, we can write $A = UDU^{*}$ because $A$ is symmetric, which means it's unitary and for unitary matrices $U^{*} = U^{-1}$. Then, we have $D = U^{*} AU$, so in $y = U^{-1} x$, the quadratic form has diagonal matrix. 

Geometrically, the columns of $U$ form an orthonormal basis in $\F^{n}$, which we'll call $B$. The change of coordinate matrix $[I]_{S, B}$ from $B$ to the standard basis is exactly $U$. Since $y = U^{-1} x$, the coordinates $y_{1}, \cdots, y_{n}$ can be interpreted as coordinates of the vector $x$ in the new basis $B$. So, orthogonal diagonalization allows us to visualize the set $Q[x] = 1$ very well if we can visualize it for diagonal matrices. 

\textbf{Example:} Consider $Q[x, y] = 2x^{2} + 2y^{2} + 2xy$. We want to describe the set of points $(x, y) \in \R^{2} : Q(x, y) = 1$. 

The matrix $A$ of $Q$ is 
$$A = \begin{bmatrix}
2 & 1 \\
1 & 2
\end{bmatrix}$$ 
After orthogonally diagonalizing $A$, we can write 
$$A = U \begin{bmatrix}
3 & 0 \\
0 & 3
\end{bmatrix} U^{*} \qquad \text{ where } U = \frac{1}{\sqrt{2}} \begin{bmatrix}
1 & -1 \\
1 & 1
\end{bmatrix}$$
which means
$$U^{*} AU = \begin{bmatrix}
3 & 0 \\
0 & 1
\end{bmatrix} = D$$

This tells us the set $\{y : (Dy, y) = 1\}$ is the ellipse with half-axes $\frac{1}{\sqrt{3}}$ and 1. Thus, the set $\{x \in \R^{2}: (Ax, x) = 1 \}$ is the same ellipse but in the basis $\begin{bmatrix}
\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}}
\end{bmatrix}, \begin{bmatrix}
-\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}}
\end{bmatrix}$, or in other words, the same ellipse rotated $\frac{\pi}{4}$. 

\subsection{Non-orthogonal Diagonalization} 

Orthogonal diagonalization requires computing eigenvalues and eigenvectors which can be hard to do for large $n$ without computers. Non-orthogonal diagonalization requires only finding an invertible $S$ without requiring $S^{-1} = S^{*}$ such that $D = S^{*}AS$ is diagonal, which is much easier to do. We will cover two of the most used methods of non-orthogonal diagonalization. 

\subsubsection{Diagonalization by Completion of Squares} 

We will demonstrate this method on real quadratic forms (forms on $
R^{n}$), but it can also be used in the complex case. 

Consider again the quadratic form of two variables $Q[x] = 2x_{1}^{2} + 2x_{1} x_{2} + 2x_{2}^{2}$ (the same equation as before except we use $x_{1}, x_{2}$ instead of $x, y$). Since 
$$2 \Big( x_{1} + \frac{1}{2} x_{2} \Big)^{2} = 2 \Big( x_{1}^{2} + 2 x_{1} \frac{1}{2} x_{2} + \frac{1}{4} x_{2}^{2} \Big) = 2x_{1}^{2} + 2x_{1}x_{2} + \frac{1}{2} x_{2}^{2}$$
where the first two terms coincide with the first two terms of $Q$, we can write 
$$Q[x] = 2 \Big( x_{1} + \frac{1}{2} x_{2} \Big)^{2} + \frac{3}{2} x_{2}^{2} = 2y_{1}^{2} + \frac{3}{2} y_{2}^{2}$$
where $y_{1} = x_{1} + \frac{1}{2} x_{2}$ and $y_{2} = x_{2}$. We can use this same method for quadratic forms of more than 2 variables. Note that we can always split a product of two variables into a corresponding square using the identity, 

$$4 x_{1} x_{2} = (x_{1} + x_{2})^{2} - (x_{1} - x_{2})^{2}$$

\subsubsection{Diagonalization Using Row/Column Operations}

Our second method is to perform row operations on the matrix $A$ of the quadratic form. Unlike normal Gauss-Jordan row reduction, after each row operation, we need to perform the same column operation (because we want to ensure $S^{*} AS$ is diagonal). 

\textbf{Maybe insert an example. }

To understand why this works, realize that a row operation corresponds to left multiplying by an elementary matrix while a column operation is equivalent to right multiplying by the transpose of the same elementary matrix. Thus, performing row operations $E_{1}, \cdots, E_{n}$ along with the same column operations gives us 
$$E_{n} \cdots E_{1} A E_{1}^{*} \cdots E_{n}^{*} = EAE^{*}$$

\textbf{Prove this including the complex case. }

As for the identity matrix in the right side of the augmented matrix, we only performed row operations on it so we have 
$$E_{n} \cdots E_{1} I = EI = E$$

Now if we set $E^{*} = S$, we know $S^{*}AS$ is a diagonal matrix, and the matrix $E = S^{*}$ is the ``right half" of the transformed augmented matrix. 

A tricky operation to implement is the swapping of two rows. Consider 
$$A = \begin{bmatrix}
0 & 1 \\
1 & 0
\end{bmatrix}$$
Swapping rows 1 and 2 would diagonalize the matrix, but we would also be required to swap columns 1 and 2, which means we end up with the original matrix. A simple idea is to use row operations to get a non-zero entry on the diagonal. For example, 
$$\begin{bmatrix}[cc|cc]
0 & 1 & 1 & 0 \\
1 & 0 & 0 & 1
\end{bmatrix} \rightarrow \begin{bmatrix}[cc|cc] 
1 & 1 & 1 & \frac{1}{2} \\
1 & 0 & 0 & 1
\end{bmatrix} \rightarrow \begin{bmatrix}[cc|cc] 
1 & 0 & 1 & \frac{1}{2} \\
0 & -1 & -1 & \frac{1}{2}
\end{bmatrix}
$$
where the first step is adding half of row 2 to row 1 (and then the corresponding column operation of adding half of column 2 to column 1), and the second step is subtracting row 1 from row 2 (and then subtracting column 1 from column 2). 

\section{Silvester's Law of Inertia} 

We now know there are many different ways to diagonalize a quadratic form. For example, if we have a diagonal matrix $D$, we can take another diagonal matrix $S$ and transform $D$ to 
$$S^{*} DS = \text{diag}\{ s_{1}^{2} \lambda_{1}, \cdots, s_{n}^{2} \lambda_{n} \}$$

This transformation changes the diagonal entries of $D$, but it does not change the \textit{signs} of the entries. The following says this is always the case. 

\begin{theorem}[Silvester's Law of Inertia]
For a Hermitian matrix $A$ (for a quadratic form $Q[x] = (Ax, x)$) and any of its diagonalizations $D = S^{*} AS$, the number of positive, negative, and zero diagonal entries of $D$ depends only on $A$, but not on a particular choice of diagonalization. 
\end{theorem}

We will need some more help to prove this. 

\begin{definition}
Given an $n \times n$ Hermitian matrix $A = A^{*}$ (a quadratic form $Q[x] = (Ax, x)$ on $\F^{n}$), we call a subspace $E \subset \F^{n}$ \textbf{positive} if 
$$(Ax, x) > 0$$ 
for all $x \in E, x \neq 0$. To emphasize the role of $A$ we will say $A$-positive. There are similar definitions for $A$-negative and $A$-neutral. 
\end{definition}

\begin{lemma}
Let $D$ be a diagonal matrix. Then the number of positive and negative diagonal entries of $D$ coincides with the maximal dimensions of a $D$-positive and $D$-negative subspace, respectively. 
\end{lemma}

\begin{lproof}
By rearranging the standard basis in $\F^{n}$, we can assume that the positive diagonal entries of $D$ are the first $r_{+}$ diagonal entries. 

Now consider the subspace $E_{+}$ spanned by these $r_{+}$ coordinate vectors $e_{1}, \cdots, e_{r_{+}}$. Clearly, $E_{+}$ is a $D$-positive subspace with dimension $r_{+}$. 

Now we'll show that for any other $D$-positive subspace $E$, we have $\Dim(E) \leq r_{+}$. Consider the orthogonal projection $P$ onto $E_{+}$, 
$$P[x_{1}, \cdots, x_{n}]^{T} = [x_{1}, \cdots, x_{r_{+}}, 0, \cdots, 0]^{T}$$

Now define an operator $T: E \rightarrow E_{+}$ by 
$$Tx = Px \qquad \forall x \in E$$

In other words, since $P$ is defined on the whole space, $T$ is the restriction of $P$ to domain $E$ and target space $E_{+}$. 

Now we'll find the null space of $T$ so that we can apply the Rank Theorem. For any $x$ such that $Tx = Px = 0$, we know $x_{1} = \cdots = x_{r_{+}} = 0$, so 
$$(Dx, x) = \sum_{r_{+} + 1}^{n} \lambda_{k} x_{k}^{2} \leq 0$$
since $\lambda_{k} \leq 0$ for $k > r_{+}$. Because $x$ belongs to $D$-positive subspace $E$, the inequality only holds for $x = 0$, so $Ker(T) = \{0\}$. 

We know $Rank(T) = \Dim(Ran(T)) \leq \Dim(E_{+}) = r_{+}$ since $Ran(T) \subset	E_{+}$. By the Rank Theorem, $\Dim(Ker(T)) + Rank(T) = \Dim(E)$. But we just proved $\Dim(Ker(T)) = 0$ which means 
$$\Dim(E) = Rank(T) \leq r_{+}$$

To prove the lemma for negative entries, we just prove the above for $-D$. 
\end{lproof}

This result proves the positive and negative diagonal entries of $D$ coincide with maximal dimensions of $D$-positive and $D$-negative subspaces. The following proves Silvester's Law of Inertia. 

\begin{theorem}
Let $A$ be an $n \times n$ Hermitian matrix, and let $D = S^{*} AS$ be its diagonalization by an invertible matrix $S$. Then the number of positive and negative diagonal entries of $D$ coincides with the maximal dimensions of an $A$-positive and $A$-negative subspace, respectively. 
\end{theorem}

\begin{proof}
Since $D = S^{*}AS$ is a diagonalization of $A$, we know 
$$(Dx, x) = (S^{*}ASx, x) = (ASx, Sx)$$
which means that for any $D$-positive subspace $E$, the subspace $SE$ is an $A$-positive subspace. The same identity implies that for any $A$-positive subspace $F$, the subspace $S^{-1}F$ is $D$-positive. 

Since $S$ is an invertible transformation, $\Dim(E) = \Dim(SE)$ and $\Dim(F) = \Dim(S^{-1}F)$. Thus, for any $D$-positive subspace $E$, we can find an $A$-positive subspace ($SE$ for example) of the same dimension, and for any $A$-positive subspace $F$, we can find a $D$-positive subspace ($S^{-}F$ for example) of the same dimension. Thus, the maximal possible dimensions of an $A$-positive and a $D$-positive subspace coincide, so by the above lemma, the theorem is proved. The same reasoning supports $A$-negative subspaces. 
\end{proof}

\section{Minimax Characterization of Eigenvalues and Silvester's Criterion of Positivity}

\begin{definition}
A quadratic form is called 
\begin{itemize}
	\item \textbf{positive definite} if $Q[x] > 0 \quad \forall x \neq 0$
	\item \textbf{positive semidefinite} if $Q[x] \geq 0$
	\item \textbf{negative definite} if $Q[x] < 0 \quad \forall x \neq 0$ 
	\item \textbf{negative semidefinite} if $Q[x] \leq 0$ 
	\item \textbf{indefinite} if it takes on both positive and negative values 
\end{itemize}
\end{definition}

A Hermitian matrix $A = A^{*}$ is called positive definite if the corresponding quadratic forms $Q[x] = (Ax, x)$ is positive definite. 

\begin{theorem}
Any Hermitian matrix $A$ is positive definite if and only if its eigenvalues are all positive.
\end{theorem}

\begin{proof}
By orthogonal diagonalization, we know there is a basis in which $A$ is diagonal, and it is clear that a diagonal matrix is positive definite if its eigenvalues are all positive. 
\end{proof}

The above reasoning also applies to positive semidefinite, negative definite, negative semidefinite, and indefinite quadratic forms. 

In fact, because of Silvester's Law of Inertia, we don't need to compute eigenvalues. We can just find a non-orthogonal diagonalization and look at those diagonal entries. 

\subsection{Silvester's Criterion of Positivity}

We'll begin by looking for a simple requirement for positivity. Let's analyze the matrix 
$$\begin{bmatrix}
a & b \\
\overline{b} & c
\end{bmatrix}$$
This matrix is positive definite if and only if $a > 0$ and $det(A)= ac - \abs{b}^{2} > 0$. To see this, note that if the above conditions are met, then $c > 0$, so $trace(A) = a + c > 0$. If $\lambda_{1}, \lambda_{2}$ are eigenvalues of $A$, then their product is positive ($det(A) > 0$) and their sum is positive($trace(A) > 0$), which is only possible if both eigenvalues are positive. \textbf{Prove only if direction.}

\begin{theorem}[Silvester's Criterion of Positivity] 
A matrix $A = A^{*}$ is positive definite if and only if 
$$det(A_{k}) > 0 \qquad \forall k = 1, \cdots, n$$
where $A_{k}$ denotes the upper left $n \times n$ submatrix of $A$. 
\end{theorem}

\begin{proof}
If $A > 0$, then $A_{k} > 0 \quad \forall k$, because all of $A$'s eigenvalues are positive. 
\textbf{Finish proof!}
\end{proof}

\subsection{Minimax Characterization of Eigenvalues}

We will use the term \textbf{codimension} to denote the dimension of the orthogonal complement. For a subspace $E \subset X$, if $\Dim(X) = n$, then $\Codim(E) + \Dim(E) = n$. 

\begin{theorem}[Minimax Characterization of Eigenvalues]
Let $A = A^{*}$ be an $n \times n$ matrix, and let $\lambda_{1}, \cdots, \lambda_{n}$ be its eigenvalues in decreasing order. Then, 
$$\lambda_{k} = \max_{E: \Dim(E) = k} \quad \min_{x \in E: \norm{x} = 1} \quad (Ax, x) \quad = \quad \min_{F: \Codim(F)=k-1} \quad \max_{x \in F: \norm{x} = 1} \quad (Ax, x)$$

\end{theorem}

The first expression considers all subspaces $E$ with dimension $k$, and then for each subspace, finds the $x$ with norm 1 that results in the minimum $(Ax, x)$. Now, out of the chosen $(Ax, x)$, we pick the subspace that results in the maximal value. The second expression is defined similarly. 

An important question regarding this theorem is: why must a maximum and minimum exist? One explanation is the set $x \in E : \norm{x} = 1$ is the unit sphere, which is closed (contains its limit points) and bounded (contains a highest and lowest point). Since $Q[x] = (Ax, x)$ is a continuous function, it will have a maximum and minimum on any compact set. \textbf{Remove if the proof proves existence.}

\begin{proof}
We can pick an orthonormal basis so that matrix $A$ is diagonal, $A = diag\{ \lambda_{1}, \cdots, \lambda_{n} \}$. We will also reorder the eigenvalues so that $\lambda_{1} \geq \cdots \geq \lambda_{n}$.

We choose subspaces $E$ and $F$ such that $\Dim(E) = k$ and $\Codim(F) = k -1$. This means that $\Dim(E) + \Dim(F) = n + 1 > n$ so there exists a non-zero vector $x_{0} \in E \cap F$. After normalizing, we get $\norm{x} = 1$, and because $x$ also belongs to both $E$ and $F$, 
$$\min_{x \in E: \norm{x} = 1} (Ax, x) \leq (Ax_{0}, x_{0}) \leq \max_{x \in F: \norm{x} = 1} (Ax, x)$$

Since we only assumed dimensions of subspaces $E$ and $F$, the above inequality holds for all pairs of $E$ and $F$ with appropriate dimensions. Now we define 
$$E_{0} := \Span\{e_{1}, \cdots, e_{k} \} \qquad F_{0} := \Span \{e_{k}, \cdots, e_{n} \}$$ 

Since for any self-adjoint matrix $B$, the maximum and minimum of $(Bx, x) : \norm{x} = 1$ are the maximum and minimum eigenvalues, we get 
$$\min_{x \in E_{0} : \norm{x} = 1} (Ax, x) = \max_{x \in F_{0} : \norm{x} = 1} (Ax, x) = \lambda_{k}$$

It follows from our above inequality that for any subspace $E$ with $\Dim(E) = k$, 
$$\min_{x \in E : \norm{x} = 1} (Ax, x) \leq \max_{x \in F_{0} : \norm{x} = 1} (Ax, x) = \lambda_{k}$$
and for any subspace $F$ with $\Codim(F) = k - 1$, 
$$\max_{x \in F : \norm{x} = 1} (Ax, x) \geq \min_{x \in E_{0} : \norm{x} = 1} (Ax, x) = \lambda_{k}$$

But because on subspaces $E_{0}$ and $F_{0}$ both the maximum and minimum are $\lambda_{k}$, we know that $\min \max = \max \min = \lambda_{k}$. 
\end{proof}