 \documentclass[a4paper,10pt]{book}

%Use the following for font size 8 or 9, as 10 is the minimum provided normally
%\documentclass[8pt]{extbook}


%Compilers
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

%Graphics
\usepackage{xcolor}
\usepackage{graphicx}

%Math
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{physics}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage[xcolor]{mdframed}

%Math shortcuts 
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\M}{\mathcal{M}}
\renewcommand{\P}{\mathcal{P}}
\renewcommand{\L}{\mathcal{L}}
\DeclareMathOperator{\Span}{span}

%Augmented matrices
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
   \hskip -\arraycolsep
   \let\@ifnextchar\new@ifnextchar
   \array{#1}}
\makeatother


%Define abs
%\DeclarePairedDelimiter\abs{\lvert}{\rvert}

%Defining norm
%\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

%Defining inner product
\DeclarePairedDelimiterX{\inp}[2]{\langle}{\rangle}{#1, #2}


%Reducing margin
\usepackage[margin=2cm]{geometry}

%Header, footer
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\rhead{\leftmark}
\cfoot{\thepage} 
\renewcommand{\headrulewidth}{0pt}

%Index - not used
\usepackage{makeidx}
\makeindex



%Theorem instantiation
\definecolor{lightblue1}{RGB}{222, 243, 253}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\surroundwithmdframed[
	hidealllines = true, 
	backgroundcolor = lightblue1,
	]{theorem}

%NEW theorem environment
%\definecolor{lightblue1}{RGB}{222, 243, 253}
%\newcounter{thmcounter}
%\newmdenv[
%	hidealllines = true, 
%	backgroundcolor = lightblue1,
%]{mytheorem}
%\newenvironment{theorem}[1][\unskip]{ %takes optional argument 1, \unskip removes the space below if no optional argument
%	\begin{mytheorem}
%	\refstepcounter{thmcounter} %increment counter 
%	\textbf{Theorem \thethmcounter} %\the__counter__ gets value of counter
%	\textsl{ #1}:
%}{
%	\end{mytheorem}
%}
%\numberwithin{thmcounter}{section}
	

%Proof instantiation 
\renewenvironment{proof}{\textsl{Proof.}}{\hfill$\blacksquare$}
\definecolor{lightblue2}{RGB}{232, 245, 252}
\surroundwithmdframed[
	hidealllines = true, 
	backgroundcolor = lightblue2,
	]{proof}
	
	
%NEW proof 
%\definecolor{lightblue2}{RGB}{232, 245, 252}
%\newmdenv[
%	hidealllines = true, 
%	backgroundcolor = lightblue2, 
%]{myproof}
%\renewenvironment{proof}{
%	\bigskip 
%	\noindent
%	\begin{myproof}
%	\textit{Proof.}
%}{
%	\end{myproof}
%	\par 
%	\bigskip
%}

	
%Lemma instantiation
\definecolor{lightgreen1}{RGB}{224, 255, 193}
\theoremstyle{plain}
\newtheorem{lemma}[theorem]{Lemma}
\surroundwithmdframed[
	hidealllines = true, 
	backgroundcolor = lightgreen1,
	]{lemma}
	
	
%NEW lemma 
%\definecolor{lightgreen1}{RGB}{224, 255, 193}
%\newmdenv[
%	hidealllines = true, 
%	backgroundcolor = lightgreen1,
%]{mylemma}
%\newenvironment{lemma}[1][\unskip]{ %takes optional argument 1, \unskip removes the space below if no optional argument
%	\bigskip 
%	\noindent
%	\begin{mylemma}
%	\refstepcounter{thmcounter} %increment counter 
%	\textbf{Lemma \thethmcounter} %\the__counter__ gets value of counter
%	\textsl{ #1}:
%}{
%	\end{mylemma}
%	\par 
%	\bigskip
%}
%\numberwithin{thmcounter}{section}

%Lemma Proof instantiation 
\newenvironment{lproof}{\textsl{Proof.}}{\hfill$\blacksquare$}
\definecolor{lightgreen2}{RGB}{232, 249, 214}
\surroundwithmdframed[
	hidealllines = true, 
	backgroundcolor = lightgreen2,
	]{lproof}
	

%NEW lproof 
%\definecolor{lightgreen2}{RGB}{232, 249, 214}
%\newmdenv[
%	hidealllines = true, 
%	backgroundcolor = lightgreen2,
%]{mylproof}
%\newenvironment{lproof}{
%	\bigskip 
%	\noindent
%	\begin{mylproof}
%	\textit{Proof.}
%}{
%	\end{mylproof}
%	\par 
%	\bigskip
%}


%Definition instantiation
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\definecolor{lightgreen}{RGB}{219, 255, 188}
\definecolor{subtlegray}{RGB}{248, 248, 248}
\surroundwithmdframed[
	hidealllines = true, 
	backgroundcolor = subtlegray,
	]{definition}
		
	
%Remove auto-indentation
\setlength{\parindent}{0cm}

%Preferred font and spacing
\linespread{1.3}
%\usepackage{lmodern}
\usepackage{kpfonts}
%\usepackage{tgschola}

\begin{document}

\frontmatter 
{\let\cleardoublepage\clearpage 
%Fancy shmancy title page - code from https://en.wikibooks.org/wiki/LaTeX/Title_Creation
\begin{titlepage}
	\centering
	\vspace{1cm}
	{\scshape\LARGE University of California, Berkeley \par}
	\vspace{4cm}
	{\scshape\Large Literally everything I know about \par}
	\vspace{1.5cm}
	{\Huge\bfseries Linear Algebra\par}
	\vspace{1cm}
	\vspace{2.5cm}
	{\Large\itshape Warren Alphonso\par}
	\vfill
	{\large A very reductionist summary of \textsc{Linear Algebra and its Applications} by Lay, Lay, and McDonald, \textsc{Linear Algebra Done Wrong} by Treil, and \textsc{Quantum Computation and Quantum Information} by Nielsen and Chuang. \par}
	\vfill
	{\large \today\par}
\end{titlepage}

\tableofcontents
}

\mainmatter




















\chapter{Outer Product Representation} 

This chapter is largely adapted from Chapter 2 of Nielsen and Chuang. Much of this chapter will be spent drawing analogies to the previous two chapters. This is intentional, as it seems that fully understanding the core properties and intuitions behind many of the ideas in the two preceding chapters is critical. 

\section{Dirac Notation} 

We can use inner products to derive a useful representations of linear operators. Before doing so, we'll define a convenient notation. 

\begin{definition}
The \textbf{Dirac notation} represents vectors as 
$$\ket*{\psi}$$ which is also known as a \textbf{ket}. The dual of this same vector (dual spaces will be defined later in the book) is represented as 
$$\bra*{\psi}$$ which is also known as a \textbf{bra}. For now, it suffices to know that the dual of a vector is simply the complex conjugate transpose of that vector. This notations allows us to represent the inner product of two vectors as 
$$\bra*{\phi}\ket*{\psi}$$
\end{definition}

Using this new notation, we can define the outer product. 

\begin{definition}
The \textbf{outer product representation} is a representation of linear operators which uses the inner product. Suppose $\ket*{v}$ is a vector in inner product space $V$, and $\ket*{w}$ is a vector in inner product space $W$. We define $\ket{w} \bra*{v}$ to be the \textit{linear operator} from $V$ to $W$ defined by 
$$\Big(\ket*{w} \bra*{v} \Big) \ket*{v_{1}} \equiv \ket*{w} \bra*{v} \ket*{v_{1}} = \bra*{v} \ket*{v_{1}} \ket*{w}$$
\end{definition}

To make this representation feel less abstract, we consider an important result using the outer product. 

\begin{lemma}[Completeness Relation]
For an orthonormal basis $\ket*{i}$, 
$$\sum_{i} \ket*{i} \bra*{i} = I$$
\end{lemma}

\begin{lproof}
We know that for any $v \in V$ can be written as $v = \sum_{i} v_{i} \ket*{i}$ and that $\bra*{i} \ket*{v} = v_{i}$. Then 

$$\Big( \sum_{i} \ket*{i} \bra*{i} \Big) \ket*{v} = \sum_{i} \ket*{i} \bra*{i} \ket*{v} = \sum_{i} v_{i} \ket*{i} = \ket*{v}$$

Because this is true for all $\ket*{v}$, it must be the identity operator. 
\end{lproof}

One application of the Completeness Relation is the representation of any operator in the outer product notation. Suppose $A: V \rightarrow W$ is a linear operator and $\ket*{v_{i}}$ and $\ket*{w_{j}}$ are orthonormal bases for $V$ and $W$, respectively. By using the Completeness Relation twice, we get
$$A = I_{W} A I_{V} = \sum_{ij} \ket*{w_{j}} \bra*{w_{j}} A \ket*{v_{i}} \bra*{v_{i}} = \sum_{ij} \bra*{w_{j}} A \ket*{v_{i}} \ket*{w_{j}} \bra*{v_{i}}$$
which is the \textbf{outer product representation of $A$}. This equation also shows that $A$ has matrix element $\bra*{w_{j}}A\ket*{v_{i}}$ in the $i$th column and $j$th row, with respect to input basis $\ket*{v_{i}}$ and output basis $\ket*{w_{j}}$.

To gain more familiarity with the outer product representation, we will prove the Cauchy-Schwarz Inequality. 

\begin{theorem}[Cauchy-Schwarz Inequality]
For any two vectors $\ket*{v}$ and $\ket*{w}$, $$\abs{\bra*{v}\ket*{w}}^{2} \leq \bra*{v}\ket*{v} \bra*{w}\ket*{w}$$
\end{theorem}

\begin{proof}
We use Gram-Schmidt to obtain an orthonormal basis $\ket{i}$, such that the first member of the basis is $\ket*{w} / \sqrt{\bra*{w}\ket*{w}}$. Then 
$$
\begin{aligned} 
\bra*{v}\ket*{v} \bra*{w}\ket*{w} &= \sum_{i} \bra*{v}\ket*{i} \bra*{i}\ket*{v} \bra*{w}\ket*{w} \\
&\geq \frac{\bra*{v}\ket*{w} \bra*{w}\ket*{v}}{\bra*{w}\ket*{w}} \bra*{w}\ket*{w} \\ 
&= \bra*{v}\ket*{w} \bra*{w}\ket*{v} = \abs{\bra*{v}\ket*{w}}^{2}
\end{aligned}
$$

where the inequality in the second step follows from the fact that we only use the first member of our basis and thus drop some non-negative terms.
\end{proof}

\section{Diagonalization}

The spectral theorem for normal operators proved in the previous chapter ($N = UDU^{\dagger}$ where $U$ is a unitary matrix of eigenvectors and $D$ is a diagonal matrix of corresponding eigenvalues) states an operator is diagonalizable if and only if it is normal. We can also write the diagonal representation of a normal matrix in outer product representation.  

\begin{theorem}
A diagonal representation for a normal operator $N$ is 
$$N = \sum_{i}^{n} \lambda_{i} \ket*{i} \bra*{i}$$
where $\ket*{i}$ are an orthonormal set of eigenvectors with corresponding eigenvalues $\lambda_{i}$. 
\end{theorem}

\begin{proof}
Let $\ket*{e}$ be an orthonormal basis of $N$. Since each basis vector $\ket*{e}$ can be understood as providing a coordinate of $\ket*{v}$ in that basis through $\bra*{e}\ket*{v}$, we know $\bra*{e}\ket*{e}$ is the 0 matrix except with a 1 on the $e$th diagonal entry. Using the spectral theorem, we get 
$$N = UDU^{\dagger} = U \Bigg( \sum_{i=1}^{n} \lambda_{i} \ket*{e}\bra*{e} \Bigg) U^{\dagger} = \sum_{i=1}^{n} \lambda_{i} U \ket*{e} \Bigg( U \ket*{e} \Bigg)^{\dagger}$$

Now we define $U \ket*{e}$ to be the normalized eigenvectors $\ket*{i}$ to get 
$$N = \sum_{i=1}^{n} \lambda_{i} \ket*{i} \bra*{i}$$
\end{proof}

As an example, we will introduce the Pauli $Z = \begin{bmatrix}
1 & 0 \\
0 & -1
\end{bmatrix}$ matrix, which is significant in quantum computation, and write its diagonal representation.  

The characteristic equation $det(A - \lambda I)$ determines that the eigenvalues for $Z$ are 1 and -1. The corresponding eigenvectors are $\begin{bmatrix}
1 \\ 0
\end{bmatrix}$ and $\begin{bmatrix}
0 \\ 1
\end{bmatrix}$, which we will denote $\ket*{0}$ and $\ket*{1}$, respectively. This means our diagonal representation is 
$$Z = \begin{bmatrix}
1 & 0 \\
0 & -1
\end{bmatrix} = \ket*{0}\bra*{0} - \ket*{1}\bra*{1}$$

\section{Adjoints and Hermitian Operators} 

Recall, if $A$ is a linear operator on $V$, then there exists a unique linear operator $A^{\dagger}$ (synonymous with $A^{*}$ in this book) on $V$ such that for all vectors $\ket*{v}, \ket*{w} \in V$, 
$$(\ket*{v}, A\ket*{w}) = (A^{\dagger}\ket*{v}, \ket*{w})$$

This linear operator is the \textbf{adjoint} or \textbf{Hermitian conjugate} of $A$. Recall earlier that we also defined $\ket*{v}^{\dagger} \equiv \bra*{v}$.

\begin{definition}
A mapping $f: V \rightarrow W$ between complex vector spaces is \textbf{antilinear} if 
$$f(ax + by) = \overline{a} f(x) + \overline{b} f(y)$$

From Wikipedia: ``antilinear maps occur in quantum mechanics in the study of time reversal and in spinor calculus."
\end{definition}

\begin{lemma}[Antilinearity of the adjoint]
$$\Bigg( \sum_{i} a_{i} A_{i} \Bigg)^{\dagger} = \sum_{i} a_{i}^{*} A_{i}^{\dagger}$$
\end{lemma}

\begin{lproof}
On the LHS: we know $(a + b)^{*} = a^{*} + b^{*}$. Thus, $(\sum_{i} a_{i}A_{i})^{\dagger} = \sum_{i} (a_{i}A_{i})^{\dagger}$.

On the RHS: $\sum_{i} a_{i}^{*} A_{i}^{\dagger} = (a_{1} A_{1})^{\dagger} + \cdots + (a_{n} A_{n})^{\dagger} = \sum_{i} (a_{i} A_{i})^{\dagger}$. 
\end{lproof}

Recall, an operator $A$ whose adjoint is itself is known as a \textbf{Hermitian} or \textbf{self-adjoint} operator. 

An interesting usage of Hermitian operators that hint at their importance is the following theorem. 

\begin{theorem}
An arbitrary operator $A$ can be decomposed into the sum $B + iC$ where $B$ and $C$ are Hermitian.
\end{theorem}

\begin{proof}
Define $B = \frac{A + A^{\dagger}}{2}$ and $C = \frac{A - A^{\dagger}}{2i}$. 

To prove $B$ is Hermitian: $B^{\dagger} = \frac{A^{\dagger} + A}{2} = B$. To prove $C$ is Hermitian: $C^{\dagger} = \frac{A^{\dagger} - A}{-2i} = C$. 

Finally, 
$$B + iC = \frac{A + A^{\dagger}}{2} + i \frac{A - A^{\dagger}}{2i} = \frac{A + A}{2} = A$$
\end{proof}

Recall, \textit{projector} operators are Hermitian operators. We can demonstrate this using outer product representation. 

Suppose $W$  is a $k$-dimensional vector \textit{subspace} of the $d$-dimensional vector space $V$. We use Gram-Schmidt to construct an orthonormal basis $\ket*{1}, \cdots, \ket*{d}$ for $V$ such that $\ket*{1}, \cdots, \ket*{k}$ is an orthonormal basis for $W$. Then, we define 
$$P = \sum_{i=1}^{k} \ket*{i} \bra*{i}$$
to be the projector onto subspace $W$. Notice that the Completeness Relation states that if $k = d$, $P = I$, which is exactly what a projector from $V \rightarrow V$ would be. This outer product representation of $P$ makes it clear that $P$ is Hermitian: since $(\ket*{v} \bra*{v})^{\dagger} = (\ket*{v} \bra*{v})$, antilinearity means
$$P^{\dagger} = \Bigg( \sum_{i}^{k} \ket*{i} \bra*{i} \Bigg)^{\dagger} = \sum_{i}^{k} (\ket*{i} \bra*{i}) = \sum_{i}^{k} \ket*{i} \bra*{i} = P$$

The Completeness Relation also makes it clear that the orthogonal complement of $P$ is defined as $Q = I - P$, and that $Q$ is a projector onto the vector space spanned by $\ket*{k + 1}, \cdots, \ket*{d}$. 

Recall, an operator $A$ is \textbf{normal} if $AA^{\dagger} = A^{\dagger}A$. Clearly, a Hermitian operator is also normal. 

Recall, an operator $U$ is \textbf{unitary} if $U^{\dagger}U = I$, so unitary operators are also normal. In the Chapter 5, we proved unitary operators preserve the norm, but we can generalize this to prove that they also preserve inner products. 

\begin{theorem}
Let $U$ be unitary. Then 
$$(U \ket*{v}, U \ket*{w}) = (\ket*{v}, \ket*{w})$$
\end{theorem}

\begin{proof}
$$ (U \ket*{v}, U \ket*{w}) = \bra*{v} U^{\dagger} U \ket*{w} = \bra*{v} I \ket*{w} = \bra*{v}\ket*{w}$$
\end{proof}

This result allows an elegant outer product representation of $U$. Let $\ket*{v_{i}}$ be an orthonormal basis. We define $$\ket*{w_{i}} = U \ket*{v_{i}}$$ so that $\ket*{w_{i}}$ is also an orthonormal basis because unitary operators preserve inner products. Right multiplying both sides by $\bra*{v_{i}}$ and using the Completeness Relation, we get 
$$\ket*{w_{i}} \bra*{v_{i}} = U \ket*{v_{i}} \bra*{v_{i}} \rightarrow U = \sum_{i} \ket*{w_{i}} \bra*{v_{i}}$$

Recall, an operator $A$ such that $(\ket*{v}, A\ket*{v})$ is real and non-negative is called a \textbf{positive operator} or positive semidefinite. If this same inner product is \textit{strictly} positive for all $\ket*{v} \neq 0$, then $A$ is \textbf{positive definite}. 

\begin{theorem}[Hermiticity of positive operators] 
A positive operator $A$ is Hermitian. 
\end{theorem} 

\begin{proof}
Since we can write $A = B + iC$ where $B$ and $C$ are Hermitian, we can use definition of a positive operator to write 
$$
\begin{aligned}
(\ket*{v}, A \ket*{v}) &= (\ket*{v}, B\ket*{v} +iC\ket*{v}) \\ 
&= (\ket*{v}, B\ket*{v}) + (\ket*{v}, iC\ket*{v}) \\
&= (B\ket*{v}, \ket*{v}) + (-iC\ket*{v}, \ket*{v}) 
\end{aligned}
$$

This implies that $A = B - iC$, so  
$$A^{\dagger} = (B - iC)^{\dagger} = B^{\dagger} + i C^{\dagger} = B + iC = A$$
\end{proof}


\end{document}