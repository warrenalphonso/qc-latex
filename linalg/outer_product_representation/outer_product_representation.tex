\chapter{Outer Product Representation} 

This chapter is largely adapted from Chapter 2 of Nielsen and Chuang. Much of it will be spent providing an additional perspective on the core ideas in the previous two chapters. New concepts, like tensor products, that are introduced in the second half of this chapter will be developed more formally later. 

\section{Dirac Notation} 

We can use inner products to derive a useful representations of linear operators. Before doing so, we'll define a convenient notation. 

\begin{definition}
The \textbf{Dirac notation} represents vectors as 
$$\ket*{\psi}$$ which is also known as a \textbf{ket}. The dual of this same vector (dual spaces will be defined later) is represented as 
$$\bra*{\psi}$$ which is also known as a \textbf{bra}. For now, it suffices to know that the dual of a vector is simply the complex conjugate transpose of that vector. This notations allows us to represent the inner product of two vectors as 
$$\bra*{\phi}\ket*{\psi}$$
\end{definition}

Using this new notation, we can define the outer product. 

\begin{definition}
The \textbf{outer product representation} is a representation of linear operators which uses the inner product. Suppose $\ket*{v}$ is a vector in inner product space $V$, and $\ket*{w}$ is a vector in inner product space $W$. We define $\ket{w} \bra*{v}$ to be the \textit{linear operator} from $V$ to $W$ defined by 
$$\Big(\ket*{w} \bra*{v} \Big) \ket*{v_{1}} \equiv \ket*{w} \bra*{v} \ket*{v_{1}} = \bra*{v} \ket*{v_{1}} \ket*{w}$$
\end{definition}

To make this representation feel less abstract, we consider an important result using the outer product. 

\begin{lemma}[Completeness Relation]
For an orthonormal basis $\ket*{i}$, 
$$\sum_{i} \ket*{i} \bra*{i} = I$$
\end{lemma}

\begin{lproof}
We know any $v \in V$ can be written as $v = \sum_{i} v_{i} \ket*{i}$ and that $\bra*{i} \ket*{v} = v_{i}$. Then 

$$\Big( \sum_{i} \ket*{i} \bra*{i} \Big) \ket*{v} = \sum_{i} \ket*{i} \bra*{i} \ket*{v} = \sum_{i} v_{i} \ket*{i} = \ket*{v}$$

Because this is true for all $\ket*{v}$, it must be the identity operator. 
\end{lproof}

One application of the Completeness Relation is the representation of any operator in the outer product notation. Suppose $A: V \rightarrow W$ is a linear operator and $\ket*{v_{i}}$ and $\ket*{w_{j}}$ are orthonormal bases for $V$ and $W$, respectively. By using the Completeness Relation twice, we get
$$A = I_{W} A I_{V} = \sum_{ij} \ket*{w_{j}} \bra*{w_{j}} A \ket*{v_{i}} \bra*{v_{i}} = \sum_{ij} \bra*{w_{j}} A \ket*{v_{i}} \ket*{w_{j}} \bra*{v_{i}}$$
which is the \textbf{outer product representation} of $A$. This equation also shows that $A$ has matrix element $\bra*{w_{j}}A\ket*{v_{i}}$ in the $i$th column and $j$th row, with respect to input basis $\ket*{v_{i}}$ and output basis $\ket*{w_{j}}$.

To gain more familiarity with the outer product representation, we will prove the Cauchy-Schwarz Inequality. 

\begin{theorem}[Cauchy-Schwarz Inequality]
For any two vectors $\ket*{v}$ and $\ket*{w}$, $$\abs{\bra*{v}\ket*{w}}^{2} \leq \bra*{v}\ket*{v} \bra*{w}\ket*{w}$$
\end{theorem}

\begin{proof}
We use Gram-Schmidt to obtain an orthonormal basis $\ket{i}$, such that the first member of the basis is $\ket*{w} / \sqrt{\bra*{w}\ket*{w}}$. Then 
$$
\begin{aligned} 
\bra*{v}\ket*{v} \bra*{w}\ket*{w} &= \sum_{i} \bra*{v}\ket*{i} \bra*{i}\ket*{v} \bra*{w}\ket*{w} \\
&\geq \frac{\bra*{v}\ket*{w} \bra*{w}\ket*{v}}{\bra*{w}\ket*{w}} \bra*{w}\ket*{w} \\ 
&= \bra*{v}\ket*{w} \bra*{w}\ket*{v} = \abs{\bra*{v}\ket*{w}}^{2}
\end{aligned}
$$

where the inequality in the second step follows from the fact that we only use the first member of our basis and thus drop some non-negative terms.
\end{proof}

\section{Diagonalization}

The spectral theorem for normal operators proved in the previous chapter ($N = UDU^{\dagger}$ where $U$ is a unitary matrix of eigenvectors and $D$ is a diagonal matrix of corresponding eigenvalues) states an operator is diagonalizable if and only if it is normal. We can also write the diagonal representation of a normal matrix in outer product representation.  

\begin{theorem}
A diagonal representation for a normal operator $N: V \rightarrow V$ is 
$$N = \sum_{i}^{n} \lambda_{i} \ket*{i} \bra*{i}$$
where $\ket*{i}$ are an orthonormal set of eigenvectors of $V$ with corresponding eigenvalues $\lambda_{i}$. 
\end{theorem}

\begin{proof}
Let $\ket*{e}$ be an orthonormal basis of $N$. Since each basis vector $\ket*{e}$ can be understood as providing a coordinate of $\ket*{v}$ in that basis through $\bra*{e}\ket*{v}$, we know $\bra*{e}\ket*{e}$ is the 0 matrix except with a 1 on the $e$th diagonal entry. Using the spectral theorem, we get 
$$N = UDU^{\dagger} = U \Bigg( \sum_{i=1}^{n} \lambda_{i} \ket*{e}\bra*{e} \Bigg) U^{\dagger} = \sum_{i=1}^{n} \lambda_{i} U \ket*{e} \Bigg( U \ket*{e} \Bigg)^{\dagger}$$

Now we define $U \ket*{e}$ to be the normalized eigenvectors $\ket*{i}$ to get 
$$N = \sum_{i=1}^{n} \lambda_{i} \ket*{i} \bra*{i}$$
\end{proof}

As an example, we will introduce the Pauli $Z = \begin{bmatrix}
1 & 0 \\
0 & -1
\end{bmatrix}$ matrix, which is significant in quantum computation, and write its diagonal representation.  

The characteristic equation $det(A - \lambda I)$ determines that the eigenvalues for $Z$ are 1 and -1. The corresponding eigenvectors are $\begin{bmatrix}
1 \\ 0
\end{bmatrix}$ and $\begin{bmatrix}
0 \\ 1
\end{bmatrix}$, which we will denote $\ket*{0}$ and $\ket*{1}$, respectively. This means our diagonal representation is 
$$Z = \begin{bmatrix}
1 & 0 \\
0 & -1
\end{bmatrix} = \ket*{0}\bra*{0} - \ket*{1}\bra*{1}$$

\section{Adjoints and Hermitian Operators} 

Recall, if $A$ is a linear operator on $V$, then there exists a unique linear operator $A^{\dagger}$ (synonymous with $A^{*}$ in quantum mechanics notation which we are adopting for this chapter) on $V$ such that for all vectors $\ket*{v}, \ket*{w} \in V$, 
$$(\ket*{v}, A\ket*{w}) = (A^{\dagger}\ket*{v}, \ket*{w})$$

This linear operator is the \textbf{adjoint} or \textbf{Hermitian conjugate} of $A$. Recall earlier that we also defined $\ket*{v}^{\dagger} \equiv \bra*{v}$.

\begin{definition}
A mapping $f: V \rightarrow W$ between complex vector spaces is \textbf{antilinear} if 
$$f(ax + by) = \overline{a} f(x) + \overline{b} f(y)$$

From Wikipedia: ``antilinear maps occur in quantum mechanics in the study of time reversal and in spinor calculus."
\end{definition}

\begin{lemma}[Antilinearity of the adjoint]
$$\Bigg( \sum_{i} a_{i} A_{i} \Bigg)^{\dagger} = \sum_{i} a_{i}^{*} A_{i}^{\dagger}$$
\end{lemma}

\begin{lproof}
On the LHS: we know $(a + b)^{*} = a^{*} + b^{*}$. Thus, $(\sum_{i} a_{i}A_{i})^{\dagger} = \sum_{i} (a_{i}A_{i})^{\dagger}$.

On the RHS: $\sum_{i} a_{i}^{*} A_{i}^{\dagger} = (a_{1} A_{1})^{\dagger} + \cdots + (a_{n} A_{n})^{\dagger} = \sum_{i} (a_{i} A_{i})^{\dagger}$. 
\end{lproof}

Recall, an operator $A$ whose adjoint is itself is known as a \textbf{Hermitian} or \textbf{self-adjoint} operator. 

An interesting usage of Hermitian operators that hint at their importance is the following theorem. 

\begin{theorem}
An arbitrary operator $A$ can be decomposed into the sum $B + iC$ where $B$ and $C$ are Hermitian.
\end{theorem}

\begin{proof}
Define $B = \frac{A + A^{\dagger}}{2}$ and $C = \frac{A - A^{\dagger}}{2i}$. 

To prove $B$ is Hermitian: $B^{\dagger} = \frac{A^{\dagger} + A}{2} = B$. To prove $C$ is Hermitian: $C^{\dagger} = \frac{A^{\dagger} - A}{-2i} = C$. 

Finally, 
$$B + iC = \frac{A + A^{\dagger}}{2} + i \frac{A - A^{\dagger}}{2i} = \frac{A + A}{2} = A$$
\end{proof}

Recall, \textit{projector} operators are Hermitian operators. We can demonstrate this using outer product representation. 

Suppose $W$  is a $k$-dimensional vector \textit{subspace} of the $d$-dimensional vector space $V$. We use Gram-Schmidt to construct an orthonormal basis $\ket*{1}, \cdots, \ket*{d}$ for $V$ such that $\ket*{1}, \cdots, \ket*{k}$ is an orthonormal basis for $W$. Then, we define 
$$P = \sum_{i=1}^{k} \ket*{i} \bra*{i}$$
to be the projector onto subspace $W$. Notice that the Completeness Relation states that if $k = d$, $P = I$, which is exactly what a projector from $V \rightarrow V$ would be. This outer product representation of $P$ makes it clear that $P$ is Hermitian: since $(\ket*{v} \bra*{v})^{\dagger} = (\ket*{v} \bra*{v})$, antilinearity means
$$P^{\dagger} = \Bigg( \sum_{i}^{k} \ket*{i} \bra*{i} \Bigg)^{\dagger} = \sum_{i}^{k} (\ket*{i} \bra*{i}) = \sum_{i}^{k} \ket*{i} \bra*{i} = P$$

The Completeness Relation also makes it clear that the orthogonal complement of $P$ is defined as $Q = I - P$, and that $Q$ is a projector onto the vector space spanned by $\ket*{k + 1}, \cdots, \ket*{d}$. 

Recall, an operator $A$ is \textbf{normal} if $AA^{\dagger} = A^{\dagger}A$. Clearly, a Hermitian operator is also normal. 

Recall, an operator $U$ is \textbf{unitary} if $U^{\dagger}U = I$, so unitary operators are also normal. In the Chapter 5, we proved unitary operators preserve the norm, but we can generalize this to prove that they also preserve inner products. 

\begin{theorem}
Let $U$ be unitary. Then 
$$(U \ket*{v}, U \ket*{w}) = (\ket*{v}, \ket*{w})$$
\end{theorem}

\begin{proof}
$$ (U \ket*{v}, U \ket*{w}) = \bra*{v} U^{\dagger} U \ket*{w} = \bra*{v} I \ket*{w} = \bra*{v}\ket*{w}$$
\end{proof}

This result allows an elegant outer product representation of $U$. Let $\ket*{v_{i}}$ be an orthonormal basis. We define $$\ket*{w_{i}} = U \ket*{v_{i}}$$ so that $\ket*{w_{i}}$ is also an orthonormal basis because unitary operators preserve inner products. Right multiplying both sides by $\bra*{v_{i}}$ and using the Completeness Relation, we get 
$$\ket*{w_{i}} \bra*{v_{i}} = U \ket*{v_{i}} \bra*{v_{i}} \rightarrow U = \sum_{i} \ket*{w_{i}} \bra*{v_{i}}$$

Recall, an operator $A$ such that $(\ket*{v}, A\ket*{v})$ is real and non-negative is called a \textbf{positive operator} or positive semidefinite. If this same inner product is \textit{strictly} positive for all $\ket*{v} \neq 0$, then $A$ is \textbf{positive definite}. 

\begin{theorem}[Hermiticity of positive operators] 
A positive operator $A$ is Hermitian. 
\end{theorem} 

\begin{proof}
Since we can write $A = B + iC$ where $B$ and $C$ are Hermitian, we can use definition of a positive operator to write 
$$
\begin{aligned}
(\ket*{v}, A \ket*{v}) &= (\ket*{v}, B\ket*{v} +iC\ket*{v}) \\ 
&= (\ket*{v}, B\ket*{v}) + (\ket*{v}, iC\ket*{v}) \\
&= (B\ket*{v}, \ket*{v}) + (-iC\ket*{v}, \ket*{v}) 
\end{aligned}
$$

This implies that $A = B - iC$, so  
$$A^{\dagger} = (B - iC)^{\dagger} = B^{\dagger} + i C^{\dagger} = B + iC = A$$
\end{proof}

\section{Tensor Products}

Tensor products are probably the most unintuitive Linear Algebra concept I've come across thus far. This section is a very top-level overview of the subject; later chapters will cover this topic in more detail. 

\begin{definition}
Suppose $V$ and $W$ are $m$ and $n$ dimensional vector spaces, respectively. Then $V \otimes W$ is the \textbf{tensor product} of these two spaces and is an $mn$ dimensional vector space. The elements of $V \otimes W$ are linear combinations of \textbf{tensor products} $\ket*{v} \otimes \ket*{w}$. If $\ket*{i}$ and $\ket*{j}$ are orthonormal bases for spaces $V$ and $W$, then $\ket*{i} \otimes \ket*{j}$ is a basis for $V \otimes W$. 
\end{definition}

The tensor product satisfies the following properties: 
\begin{enumerate}
	\item For scalar $z$, $\ket*{v} \in V$, and $\ket*{w} \in W$, $$z (\ket*{v} \otimes \ket*{w}) = (z\ket*{v}) \otimes \ket*{w} = \ket*{v} \otimes (z\ket*{w})$$
	\item For $\ket*{v_{1}}, \ket*{v_{2}} \in V$ and $\ket*{w} \in W$, $$(\ket*{v_{1}} + \ket*{v_{2}}) \otimes \ket*{w} = \ket*{v_{1}} \otimes \ket*{w} + \ket*{v_{2}} \otimes \ket*{w}$$
	\item For $\ket*{v} \in V$ and $\ket*{w_{1}}, \ket*{w_{2}} \in W$, $$\ket*{v} \otimes (\ket*{w_{1}} + \ket*{w_{2}}) = \ket*{v} \otimes \ket*{w_{1}} + \ket*{v} \otimes \ket*{w_{2}}$$
\end{enumerate}

If $A$ and $B$ are linear operators on $V$ and $W$, respectively, then we can define a linear operator $A \otimes B$ on $V \otimes W$ as 
$$(A \otimes B)(\ket*{v} \otimes \ket*{w}) \equiv A\ket*{v} \otimes B\ket*{w}$$

We can extend this definition of $A \otimes B$ to ensure linearity of $A \otimes B$ so 
$$(A \otimes B) \Bigg( \sum_{i} \alpha_{i} \ket*{v_{i}} \otimes \ket*{w_{i}} \Bigg) \equiv \sum_{i} \alpha_{i} A \ket*{v_{i}} \otimes B \ket*{w_{i}}$$

An arbitrary linear operator $C$ mapping $V \otimes W$ to $V^{'} \otimes W^{'}$ can be represented as a linear combination of tensor products of operators mapping $A: V \rightarrow V^{'}$ and $B: W \rightarrow W^{'}$, 
$$C = \sum_{i} c_{i} A_{i} \otimes B_{i}$$

We can use the inner products on $V$ and $W$ to define an inner product on $V \otimes W$, 
$$\Bigg( \sum_{i} \alpha_{i} \ket*{v_{i}} \otimes \ket*{w_{i}} , \sum_{j} \beta_{j} \ket*{v_{j}^{'}} \otimes \ket*{w_{j}^{'}} \Bigg) \equiv \sum_{ij} \alpha_{i}^{*} b_{j} \bra*{v_{i}}\ket*{v_{j}^{'}} \bra*{w_{i}}\ket*{w_{j}^{'}}$$

To make all this less abstract, we consider a matrix representation known as the \textit{Kronecker product}. Suppose $A$ is an $m \times n$ matrix and $B$ is a $p \times q$ matrix. Then 
$$A \otimes B \equiv \overbrace{\left.\begin{bmatrix}
A_{11}B & A_{12}B & \cdots & A_{1n}B \\ 
A_{21}B & A_{22}B & \cdots & A_{2n}B \\
\vdots & \vdots & & \vdots \\ 
A_{m1}B & A_{m2}B & \cdots & A_{mn}B
\end{bmatrix} \right\}}^{nq} mp
$$

where $A_{11}B$ denotes the $p \times q$ submatrix $B$ multiplied by the constant $A_{11}$. 

For example, 
$$\begin{bmatrix}
1 \\ 2
\end{bmatrix} \otimes \begin{bmatrix}
2 \\ 3
\end{bmatrix} = \begin{bmatrix}
1 \times 2 \\
1 \times 3 \\
2 \times 2 \\
2 \times 3
\end{bmatrix} = \begin{bmatrix}
2 \\ 
3 \\ 
4 \\ 
6
\end{bmatrix}$$

and the tensor product of Pauli $X$ and $Y$ is 
$$X \otimes Y = \begin{bmatrix}
0 \cdot Y & 1 \cdot Y \\
1 \cdot Y & 0 \cdot Y
\end{bmatrix} = \begin{bmatrix}
0 & 0 & 0 & -i \\
0 & 0 & i & 0 \\
0 & -i & 0 & 0 \\
i & 0 & 0 & 0
\end{bmatrix}$$

The notation $\ket*{\psi}^{\otimes k}$ denotes $\ket*{\psi}$ tensored with itself $k$ times. 

Do problems from Nielsen and Chuang. 

\section{Operator Functions}

Given a function $f: C \rightarrow C$, we can define a corresponding matrix function. Let $A = \sum_{a} a \ket*{a} \bra*{a}$, then 
$$f(A) \equiv \sum_{a} f(a) \ket*{a} \bra*{a}$$

We can use this construction to define the square root of a positive operator, the logarithm of a positive-definite operator, or the exponential of a normal operator. 

One important matrix function is \textit{trace}. Since we know trace is \textit{cyclic} ($tr(AB) = tr(BA)$) then $tr(UAU^{\dagger}) = tr(U^{\dagger}UA) = tr(A)$. 

Suppose $\ket*{\psi}$ is a unit vector and $A$ is an arbitrary operator. To evaluate $tr(A\ket*{v} \bra*{v})$ use Gram-Schmidt to extend $\ket*{\psi}$ to an orthonormal basis $\ket*{i}$ which includes $\ket*{\psi}$ as the first element. Then 
$$tr(A \ket*{\psi} \bra*{\psi}) = \sum_{i} \bra*{i}A\ket*{\psi} \bra*{\psi}\ket*{i} = \bra*{\psi}A\ket*{\psi}$$

\section{Commutator and Anti-commutator} 

\begin{definition}
The \textbf{commutator} between two operators $A$ and $B$ is defined to be 
$$[A, B] \equiv AB - BA$$

If $[A, B] = 0$, then we say $A$ \textbf{commutes} with $B$. The \textbf{anti-commutator} is defined to be 
$$\{A, B\} \equiv AB + BA$$

We say $A$ \textit{anti-commutes} with $B$ if $\{ A, B\} = 0$.
\end{definition}

Many important properties of pairs of operators can be deduced from their commutator or anti-commutator. A useful relation is the connection between the commutator and the property of being able to \textit{simultaneously diagonalize} Hermitian operators $A$ and $B$, that is, write $A = \sum_{i} a_{i} \ket*{i} \bra*{i}, B = \sum_{i} b_{i} \ket*{i} \bra*{i}$, where $\ket*{i}$ is a common set of eigenvectors for $A$ and $B$. 

\begin{theorem}[Simultaneous diagonalization theorem]
Suppose $A$ and $B$ are Hermitian operators. Then $[A, B] = 0$ if and only if there exists an orthonormal basis such that both $A$ and $B$ are diagonal with respect to that basis. 
\end{theorem}

\begin{proof}
Proving if is simple, since if $A= UA_{1}U^{-1}$ and $B = UB_{1}U^{-1}$, then clearly $AB - BA = 0$. 

To prove only if, let $\ket*{a, j}$ be an orthonormal basis for the eigenspace $V_{a}$ of $A$ with eigenvalue $a$; the index $j$ being used to label possible degeneracies. Because we know $AB = BA$, we can write
$$AB \ket*{a, j} = BA \ket*{a, j} = aB \ket*{a, j}$$

which means $B\ket*{a, j}$ is an element of the eigenspace $V_{a}$. Let $P_{a}$ denote the projector onto $V_{a}$ and define $B_{a} = P_{a} BP_{a}$. Since projectors are Hermitian, we know $B_{a}$ is also Hermitian so it must have a spectral decomposition in terms of an orthonormal set of eigenvectors that span $V_{a}$. Let's call these eigenvectors $\ket*{a, b, k}$, where $a$ and $b$ label the eigenvalues of $A$ and $B_{a}$, and $k$ indexes degeneracies. Note that $B \ket*{a, b, k}$ is in $V_{a}$, so $B \ket*{a, b, k} = P_{a} B \ket*{a, b, k}$. Additionally, we know $P_{a} \ket*{a, b, k} = \ket*{a, b, k}$, so 
$$B \ket*{a, b, k} = P_{a} B P_{a} \ket*{a, b, k} = b \ket*{a, b, k}$$

This means $\ket*{a, b, k}$ is an eigenvector of $B$ with eigenvalue $b$, and therefore $\ket*{a, b, k}$ is a spanning orthonormal set of eigenvectors of both $A$ and $B$. Thus, $A$ and $B$ are simultaneously diagonalizable. 
\end{proof}

Another use of the commutator and anti-commutator is the following identity: 
$$AB = \frac{[A, B] + \{A, B\}}{2}$$

\begin{lemma}
Suppose $[A, B] = 0, \{A, B\} = 0,$ and $A$ is invertible. Then $B$ is 0. 
\end{lemma}

\begin{lproof}
Using the identity we just defined, we know $AB = \frac{0 + 0}{2}$. Since $A$ is invertible we left multiply both sides by $A^{-1}$ to get $B = 0$. 
\end{lproof}

Two more easily verifiable identities: 
\begin{enumerate}
	\item $[A, B]^{\dagger} = [B^{\dagger}, A^{\dagger}]$
	\item $[A, B] = - [B, A]$ 
	\item If $A, B$ are Hermitian, then $i[A, B]$ is also Hermitian. 
\end{enumerate}

\section{Polar Decomposition}

We don't understand \textit{general} linear operators incredibly well, but we do have a good understanding of \textit{unitary} and \textit{positive} operators. The polar and singular value decompositions allow us to take advantage of this by breaking general linear operators into products of unitary and positive operators. 

\begin{theorem}[Polar decomposition]
Let $A$ by a linear operator on $V$. Then there exists unitary $U$ and positive operators $J$ and $K$ such that 
$$A = UJ = KU$$
where $J = \sqrt{A^{\dagger}A}$ and $K = \sqrt{AA^{\dagger}}$. Additionally, if $A$ is invertible, then $U$ is unique.
\end{theorem}

\begin{proof}
We know $J$ (which is the modulus of $A$) is a positive operator (which we know are normal), so we can give it a spectral decomposition, 
$$J = \sum_{i} \lambda_{i} \ket*{i} \bra*{i} \qquad (\lambda_{i} \geq 0)$$

Define $\ket*{\psi_{i}} = A \ket*{i}$, so that $\bra*{\psi_{i}}\ket*{\psi_{i}} = \lambda_{i}^{2}$. 

Now consider only the positive eigenvalues. With these eigenvalues, define $\ket*{e_{i}} = \ket*{\psi_{i}} / \lambda_{i}$, so the $\ket*{e_{i}}$ are normalized and are orthogonal, since if $i \neq j$, then $\bra*{e_{i}}\ket*{e_{j}} = \bra*{i}A^{\dagger}A\ket*{j} / \lambda_{i} \lambda_{j} = 0$. 

Now we use Gram-Schmidt to extend the orthonormal set $\ket*{e_{i}}$ to form an orthonormal basis, which we'll also call $\ket*{e_{i}}$. 

We define the unitary operator $U = \sum_{i} \ket*{e_{i}} \bra*{i}$. When $\lambda_{i} \neq 0$, we have $UJ\ket*{i} = \lambda_{i} \ket*{e_{i}} = \ket*{\psi_{i}} = A \ket*{i}$. When $\lambda_{i} = 0$, we have $UJ \ket*{i} = 0 = \ket*{\psi_{i}}$. Since $A$ and $UJ$ agree on the basis $\ket*{i}$, we know $A = UJ$. 

$J$ is unique since left-multiplying $A = UJ$ by $A^{\dagger} = JU^{\dagger}$ gives $A^{\dagger}A = J^{2}$, from which we get $J = \sqrt{A^{\dagger}A}$. The equation $A = UJ$ also means that if $A$ is invertible, so is $J$, which means $U$ is uniquely determined by $U = AJ^{-1}$. 

Proving the right polar decomposition $A = KU$ follows since $$A = UJ = UJU^{\dagger}U = KU$$
where $K = UJU^{\dagger}$ is a positive operator. And since $AA^{\dagger} = KUU^{\dagger}K = K^{2}$, we have $K = \sqrt{AA^{\dagger}}$. 
\end{proof} 

\section{Singular Value Decomposition} 

The singular value decomposition combines polar decomposition and the spectral theorem. 

\begin{theorem}[Singular value decomposition]
Let $A$ be a linear operator on $V$. Then there exist unitary matrices $U$ and $V$, and a diagonal matrix $D$ with non-negative entries such that $$A = UDV$$
The diagonal entries of $D$ are called the \textit{singular values} of $A$. 
\end{theorem}

\begin{proof}
By the polar decomposition, $A = SJ$, for unitary $S$ and positive operator $J$. By the spectral theorem, $J = TDT^{\dagger}$, for unitary $T$ and diagonal $D$ with non-negative entries. 

Now we just set $U = ST$ and $V = T^{\dagger}$ to get 
$$A = SJ = STDT^{\dagger} = UDV$$
\end{proof}