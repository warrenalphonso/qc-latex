\chapter{Systems of Linear Equations}
\section{Representations of Linear Systems}
One understanding of a \textit{linear system} is simply a collection of $m$ linear equations with $n$ unknowns $x_{1}, \cdots, x_{n}$. Solving this system entails finding all $n$-tuples of numbers $x_{1}, \cdots, x_{n}$ which satisfy the $m$ equations simultaneously. If we define 
$$A = \begin{bmatrix}
\alpha_{11} & \alpha_{12} & \cdots & \alpha_{1n} \\
\alpha_{21} & \alpha_{22} & \cdots & \alpha_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
\alpha_{m1} & \alpha_{m2} & \cdots & \alpha_{mn}
\end{bmatrix}$$
then we can summarize our linear system in matrix form
$$Ax = b$$

The above is the \textbf{coefficient matrix}. If we want to contain all the information in a single matrix, we can use an \textbf{augmented matrix}
$$\begin{bmatrix}[cccc|c]
\alpha_{11} & \alpha_{12} & \cdots & \alpha_{1n} & b_{1}\\
\alpha_{21} & \alpha_{22} & \cdots & \alpha_{2n} & b_{2}\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
\alpha_{m1} & \alpha_{m2} & \cdots & \alpha_{mn} & b_{m}
\end{bmatrix}$$

\section{Solving Linear Systems}
Linear systems are solved using \textbf{Gaussian elimination}. We can perform the following row operation on an augmented matrix:
\begin{enumerate}
	\item (Replace) Replace one row by the sum of itself and a multiple of another row. 
	\item (Interchange) Interchange two rows. 
	\item (Scale) Multiply all entries in a row by a nonzero constant. 
\end{enumerate}

These operations belong to the \textit{elementary matrices}: any operation can be described by applying the same operation to $I$ to get $E$ and then multiplying $EA$.

\begin{definition}
For an augmented matrix 
$$\begin{bmatrix}[ccc|c]
1 & 2 & 3 & 1 \\
3 & 2 & 1 & 7 \\
2 & 1 & 2 & 1
\end{bmatrix}$$
the \textbf{echelon form} is 
$$\begin{bmatrix}[ccc|c]
1 & 2 & 3 & 1 \\
0 & 1 & 2 & -1 \\
0 & 0 & 2 & -4
\end{bmatrix}$$
and the \textbf{reduced echelon form} is 
$$\begin{bmatrix}[ccc|c]
1 & 0 & 0 & 1 \\
0 & 1 & 0 & 3 \\
0 & 0 & 1 & -2
\end{bmatrix}$$

Formally, the \textbf{echelon form} requires that all zero rows are below all nonzero rows and that any nonzero row's \textbf{pivot} (its leading entry) is strictly to the right of the leading entry in the previous row. The particular echelon form above is called \textbf{triangular form} and is only possible when we have a square matrix. The \textbf{reduced echelon form} requires echelon form in addition to maintaining that all pivot entries are 1 and that all entries above each pivot are 0. 
\end{definition}

The \textit{existence} and \textit{uniqueness} of a solution can be determined by analyzing pivots in the echelon form of a matrix. When looking at the coefficient matrix: 
\begin{enumerate}
	\item A solution (if it exists) is unique if and only if there are no free variables, that is if the echelon form has a pivot in every \textit{column}. 
	\item A solution is consistent if and only if the echelon form has a pivot in every \textit{row}. 
\end{enumerate}
The first statement is trivial because free variables are responsible for all non-uniqueness. For the second statement, if we have a row with no pivots in the echelon form of a matrix, we have $\begin{bmatrix}[ccc|c]
0 & \cdots & 0 & b_{k}
\end{bmatrix}$, which certainly has no solution. Thus, in order for a solution to \textit{exist} and be \textit{unique}, the echelon form must have a pivot in \textit{every column and every row}. 

\begin{theorem}
Any linearly independent system of vectors in $\mathbb{F}^{n}$ cannot have more than $n$ vectors in it. 
\end{theorem}

\begin{proof}
Let a system $v_{1}, \cdots, v_{m} \in \mathbb{F}^{n}$ be linearly independent and let $A = \begin{bmatrix}
v_{1} & \cdots & v_{m}
\end{bmatrix}$ be $n \times m$. We must show that $x_{1} v_{1} + \cdots + x_{m} v_{m}= 0$, or equivalently $Ax = 0$, has unique solution $x = 0$. According to statement 1 above, a solution can only be unique if the echelon form has a pivot in every column. This is impossible if $m > n$.  
\end{proof}

\begin{theorem}
A matrix $A$ is invertible if and only if its echelon form has pivot in every column and every row. 
\end{theorem}

\begin{proof}
Since a matrix must have unique solution for $Ax = b$ for any $b$ in order to be invertible, it is necessary that the echelon form has pivot in every column and row, according to statements 1 and 2 above. 
\end{proof}
This directly implies that an invertible matrix \textbf{must be square}.

Since an invertible matrix must be square and must have pivots in every row and column in echelon form, any invertible matrix is row equivalent to the identity matrix. We can use this to get the following algorithm for finding $A^{-1}$:

\begin{enumerate}
	\item Form an \textit{augmented $n \times 2n$} matrix 
	$\begin{bmatrix}[c|c] A & I\end{bmatrix}$.
	\item Perform row operations to transform $A$ into $I$.
	\item The matrix that was originally $I$ will now be $A^{-1}$.
\end{enumerate}

To fully understand this algorithm, remember that every row operation can be expressed as the left multiplication by an elementary matrix. Let $E = E_{n} \cdots E_{2} E_{1}$ represent all the performed row operations. Since we know $E$ transforms $A$ to the identity matrix, we have $EA = I$, so $E = A^{-1}$. Since row operations affect the entire augmented matrix, we have $\begin{bmatrix}[c|c]
A & I
\end{bmatrix} \rightarrow \begin{bmatrix}[c|c]
EA & EI
\end{bmatrix} = \begin{bmatrix}[c|c]
I & A^{-1}
\end{bmatrix}$. 

\section{Fundamental Subspaces}

\begin{definition}
A \textbf{subspace} of vector space $V$ is a non-empty subset $V_{0} \subset V$ which is also a vector space. Subspaces must be non-empty because all vector spaces must contain the zero vector. 
\end{definition}

For any linear transformation $A : V \rightarrow W$, we can associate the following subspaces: 
\begin{enumerate}
	\item The \textit{null space}, or \textit{kernel}, of $A$ which consists of all vectors $v \in V$ such that $Av = 0$. 
	\item The \textit{range} of $A$ which is the set of all vectors $w \in W$ which can be represented as $w = Av$ for $v \in V$. 
\end{enumerate}

By the \textit{column by coordinate rule}, we know that any vector in $\Range(A)$ can be represented as a weighted sum of the column vectors of $A$, which is why the term \textit{column space} is sometimes used to refer to range. 

In addition, we can consider the corresponding subspaces of the transposed matrix. The term \textit{row space} is used to denote $\Range(A^{T})$, and the term \textit{left null space} is used to denote $\Null(A^{T})$. Together, these four subspaces are known as the \textbf{fundamental subspaces} of the matrix $A$. 

\begin{definition}
The \textbf{dimension} of a vector space $V$, denoted $\Dim(V)$, is the number of vectors in a basis. 
\end{definition}

\begin{theorem}[General solution of a linear equation] 
Let a vector $x_{1}$ denote a solution to the equation $Ax = b$, and let $H$ be the set of all solutions of $Ax = 0$. Then the set
$$x = x_{1} + x_{h} : x_{h} \in H$$
is the set of \textit{all solutions} of the equation $Ax = b$. 

In other words, 
$$\Big(\text{General solution of $Ax=b$}\Big) = \Big(\text{A particular solution of $Ax=b$}\Big) + \Big(\text{General solution of $Ax=0$}\Big)$$
\end{theorem}
\begin{proof}
We know $Ax_{1} = b$ and $Ax_{h} = 0$. For $x = x_{1} + x_{h}$,
$$Ax = A(x_{1} + x_{h}) = Ax_{1} + Ax_{h} = b + 0 = b$$

Therefore, any solution $x$ for $Ax = b$ can be represented as $x = x_{1} + x_{h}$ with some $x_{h} \in H$. 
\end{proof}

The power of this theorem is its generality -- it applies to all linear equations. Aside from showing the structure of the solution set, this theorem allows us to separate investigations of uniqueness from existence. To study uniqueness of a solution, we only need to analyze uniqueness of $Ax = 0$, which always has a solution. 

\begin{theorem}
In order to compute the fundamental subspaces, we need to do row reduction. Let $A$ be the original matrix and let $A_{e}$ be its echelon form. 
\begin{enumerate}
	\item The pivot \textit{columns} of the \textit{original matrix} $A$ (the columns where after row operations we will have pivots in echelon form) give us a basis for $\Range(A)$. 
	\item The pivot \textit{rows} of $A_{e}$ give us the basis in row space. 
	\item To find $\Null(A)$, we need to solve $Ax = 0$. 
\end{enumerate}
\end{theorem}

\begin{proof}
In turn, 
\begin{enumerate}
	\item We know the pivot columns of $A_{e}$ form a basis for $\Range(A_{e})$. Since $A_{e} = EA$ ($E$ is the matrix product of the elementary matrices representing the row operations completed), $A = E^{-1}A_{e}$. This means the corresponding columns in $A$ of $A_{e}$ is a basis of $A$. 
	
	\item We know that the pivot rows of the echelon form are linearly independent. Now we need only prove that they span the entirety of the row space. Notice that \textit{row operations do not change the row space}. To prove this, 
	$$A_{e} = EA$$
	where $A$ is $m \times n$ and $E$ is an $m \times m$ invertible matrix. 
	$$\Range(A_{e}^{T}) = \Range(A^{T} E^{T}) = A^{T} (\Range(E^{T})) = A^{T} (\mathbb{R}^{m}) = \Range(A^{T})$$
	where the final step follows from applying an $n \times m$ matrix to $\mathbb{R}^{m}$, which is just a transformation from $\mathbb{R}^{m}$ to $\Range(A^{T})$. 
	
	\item Solving for $Ax = 0$ certainly gives us a spanning set for  $\Null(A)$. To prove the set is linearly independent, multiply each vector by its corresponding free variable and add. For every free variable $x_{k}$, the entry $k$ is exactly $x_{k}$, so the only way the sum of the set is $0$ is if all the free variables are $0$. 
\end{enumerate}
\end{proof}

As an example of these computations, consider the matrix 
$$\begin{bmatrix}
1 & 1 & 2 & 2 & 1 \\
2 & 2 & 1 & 1 & 1 \\
3 & 3 & 3 & 3 & 2 \\
1 & 1 & -1 & -1 & 0
\end{bmatrix}$$

Performing row operations, we get the echelon form 
$$\begin{bmatrix}
1 & 1 & 2 & 2 & 1 \\
0 & 0 & -3 & -3 & -1 \\
0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0
\end{bmatrix}$$

So the first and third columns of the \textit{original matrix} give us a basis for $\Range(A)$: 
$$\begin{bmatrix}
1 \\ 
2 \\
3 \\ 
1
\end{bmatrix}\ ,\  \begin{bmatrix}
2 \\ 
1 \\
3 \\
-1
\end{bmatrix}$$

We also know the basis for the row space of $A$ is the first and second row of the \textit{echelon form}:
$$\begin{bmatrix}
1 \\
1 \\
2 \\
2 \\
1
\end{bmatrix} \ , \ \begin{bmatrix}
0 \\
0 \\ 
-3 \\ 
-3 \\ 
-1
\end{bmatrix}$$

To find $\Null(A)$ we solve $Ax = 0$. The reduced echelon form is 
$$\begin{bmatrix}
1 & 1 & 0 & 0 & \frac{1}{3} \\
0 & 0 & 1 & 1 & \frac{1}{3} \\
0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0
\end{bmatrix}$$
This means 
$$ \left\{ \begin{matrix}
x_{1} = -x_{2} - \frac{1}{3} x_{5} \\
x_{2} \text{ is free} \\
x_{3} = -x_{4} - \frac{1}{3} x_{5} \\
x_{4} \text{ is free} \\
x_{5} \text{ is free}
\end{matrix} \right\} \longrightarrow x_{2} \begin{bmatrix}
-1 \\
1 \\ 
0 \\
0 \\
0
\end{bmatrix} + x_{4} \begin{bmatrix}
0 \\ 
0 \\
-1 \\
1 \\
0
\end{bmatrix} + x_{5} \begin{bmatrix}
-\frac{1}{3} \\
0 \\
-\frac{1}{3} \\
0 \\
1
\end{bmatrix}$$

The vectors at each free variables form the basis for $\Null(A)$. 

\begin{definition}
The \textbf{rank} of a linear transformation $A$, denoted $\Rank(A)$, is the dimension of the range of $A$. 
$$\Rank(A) := \Dim(\Range(A))$$
\end{definition}

\begin{theorem}[The Rank Theorem]
For a matrix $A$ 
$$\Rank(A) = \Rank(A^{T})$$

The proof of this is trivial since rank of both column space and row space are dependent on the number of pivots in echelon form. 
\end{theorem}

\begin{theorem}[Rank-Nullity Theorem]
Let $A$ be an $m \times n$ matrix. Then 
\begin{enumerate}
	\item $\Dim(\Null(A)) + \Dim(\Range(A)) = n$ (dim of domain)
	\item $\Dim(\Null(A^{T})) + \Dim(\Range(A^{T})) = \Dim(\Null(A^{T})) + \Rank(A) = m$ (dim of codomain)
\end{enumerate}
\end{theorem}

\begin{proof}
In turn, 
\begin{enumerate}
	\item The first equality is simply that the number of free variables + the number of pivots = the number of columns. 
	\item The second equality applies the Rank Theorem to prove the row counterpart to the first equality. 
\end{enumerate}
\end{proof}

The following follows from the second statement in the above theorem. 
\begin{theorem}
Let $A$ be an $m \times n$ matrix. Then the equation 
$$Ax = b$$ 
has a solution for \textit{every} $b \in \mathbb{R}^{m}$ if and only if the \textit{dual} equation 
$$A^{T}x = 0$$ 
has only the trivial solution. 
\end{theorem}

\section{Change of Basis}
Let $V$ be a vector space with a basis $B := b_{1}, \cdots, b_{n}$. Recall that any vector $v \in V$ can be written
$$v = x_{1} b_{1} + \cdots + x_{n} b_{n}$$
where the numbers $x_{1}, \cdots, x_{n}$ are called the coordinates of $v$. We can write the \textit{coordinate vector} as 
$$[v]_{B} := \begin{bmatrix}
x_{1} \\
\vdots \\
x_{n}
\end{bmatrix} \in \mathbb{F}^{n}$$

Note that $v \rightarrow [v]_{B}$ is an isomorphism between $V$ and $\mathbb{F}^{n}$.

\begin{definition}
Let $T: V \rightarrow W$ be a linear transformation, and let $A = \{a_{1}, \cdots, a_{n}\}$, $B = \{ b_{1}, \cdots, b_{m} \}$ be bases in $V,W$ respectively. 

A \textbf{matrix of transformation $T$ in bases $A$ and $B$} is an $m \times n$ matrix, denoted by $[T]_{BA}$, 
$$[Tv]_{B} = [T]_{BA} [v]_{A}$$

The matrix $[T]_{BA}$ is easy - its $k$th column is just $[Ta_{k}]_{B}$. 
\end{definition}

\begin{definition}
For the above two bases $A$ and $B$, the \textbf{change of basis} is 
$$[v]_{B} = [I]_{BA} v_{A}$$
where $[I]_{BA}$ is the \textbf{change of basis matrix} whose $k$th column is $[a_{k}]_{B}$. 

Clearly, any change of basis is invertible and 
$$[I]_{BA} = ([I]_{AB})^{-1}$$
\end{definition}

\begin{definition}
We can use this to define \textbf{similar matrices} as matrices $A, B$ such that 
$$A = Q^{-1}BQ$$

This means we can treat similar matrices as different representations of the same linear operator. 
\end{definition}