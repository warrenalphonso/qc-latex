\chapter{Matrix Functions - WIP} 

A \textbf{matrix function} is a function that maps one matrix to another. 

\section{Matrix Exponential}

Most of this section is from the Wikipedia article on the matrix exponential and Dan Klain's notes available at 

\begin{definition}
The \textbf{matrix exponential} is a matrix function on square matrices similar to the scalar exponential function. It is used to solve systems of linear differential equations. 

Let $X$ be an $n \times n$ matrix. The exponential is given by the power series, 
$$e^{X} = \sum_{k=0}^{\infty} \frac{1}{k!} X^{k} = I + X + \frac{1}{2!} X^{2} + \frac{1}{3!} X^{3} + \cdots $$
where $X^{0}$ is the identity matrix. Note that if $X$ is a $1 \times 1$ matrix, the exponential of $X$ is a $1 \times 1$ matrix whose element is the exponential of $X$'s sole element. 
\end{definition}

Two properties that are clear from this definition are 
\begin{enumerate}
	\item $exp(0) = I$ 
	\item $exp(X^{*}) = (exp(X))^{*}$
\end{enumerate}

Another useful property is the following result.

\begin{lemma}
If $Y$ is invertible, then 
$$e^{YXY^{-1}} = Ye^{X}Y^{-1}$$
\end{lemma}

\begin{lproof}
$$
\begin{aligned}
e^{YXY^{-1}} &= I + YXY^{-1} + \frac{1}{2!} (YXY^{-1})^{2} + \frac{1}{3!} (YXY^{-1})^{3} + \cdots \\
&= I + YXY^{-1} + Y \frac{X^{2}}{2!} Y^{-1} + Y \frac{X^{3}}{3!} Y^{-1} + \cdots \\
&= Y \Big( I + X + \frac{X^{2}}{2!} + \frac{X^{3}}{3!} + \cdots \Big) Y^{-1} \\
&= Ye^{X} Y^{-1}
\end{aligned}
$$
\end{lproof}

Not all properties of the scalar exponential function translate to the matrix exponential. For example, we know $e^{a + b} = e^{a} e^{b}$ when $a$ and $b$ are scalars, but the matrix exponential version of this typically does not hold. There are a few exceptions, however, as the following lemma illustrates. 

\begin{lemma}
Let $A$ be a square matrix, and let $s, t \in \C$. Then 
$$e^{A(s + t)} = e^{As} e^{At}$$
\end{lemma} 

\begin{lproof}
$$
\begin{aligned}
e^{As} e^{At} &= \Big( I + As + \frac{A^{2} s^{2}}{2!} + \cdots \Big) \Big( I + At + \frac{A^{2} t^{2}}{2!} + \cdots \Big) \\
&= \Big( \sum_{j=0}^{\infty} \frac{A^{j} s^{j}}{j!} \Big) \Big( \sum_{k=0}^{\infty} \frac{A^{k} t^{k}}{k!} \Big) \\
&= \sum_{j=0}^{\infty} \sum_{k=0}^{\infty} \frac{A^{j + k} s^{j} t^{k}}{j! k!} \Big)
\end{aligned}
$$

If we let $n = j + k$ so that $k = n - j$, we can write 
$$e^{As} e^{At} = \sum_{j=0}^{\infty} \sum_{n=j}^{\infty} \frac{A^{n} s^{j} t^{n - j}}{j! (n - j)!} = \sum_{n=0}^{\infty} \frac{A^{n}}{n!} \sum_{j=0}^{n} \frac{n!}{j! (n - j)!} s^{j} t^{n-j} = \sum_{n=0}^{\infty} \frac{A^{n} (s + t)^{n}}{n!} = e^{A(s +t)}$$
where the second equality follows from rearranging terms and the third follows from the Binomial Theorem. 
\end{lproof}

If we set $s = 1$ and $t = -1$, we get 
$$e^{A} e^{-A} = e^{A(1 - 1)} = e^{0} = I$$
which means that regardless of what matrix $A$ is, the exponential matrix $e^{A}$ is \textbf{always} invertible, with inverse $e^{-A}$. 

An important motivation for the matrix exponential is its use in differential equations. 

\begin{theorem}
Let $A$ be a square matrix, and let $t$ be a real scalar. Let $f(t) = e^{tA}$. Then $$f'(t) = Ae^{tA}$$ 
\end{theorem}

\begin{proof}
Applying the previous lemma to the limit yields 
$$f'(t) = \lim_{h \rightarrow 0} \frac{e^{A(t+h)} - e^{At}}{h} = e^{At} \Big( \lim_{h \rightarrow 0} \frac{e^{Ah} - I}{h} \Big)$$

Expanding $e^{Ah} - I$ gives us 
$$f'(t) = e^{At} \Big( \lim_{h \rightarrow 0} \frac{1}{h} \Big[ Ah + \frac{A^{2} h^{2}}{2!} + \cdots \Big] \Big)= e^{At} \Big( \lim_{h \rightarrow 0} A + \frac{A^{2} h}{2!} + \cdots \Big) = e^{At} A = Ae^{At}$$
\end{proof}

Recall that if $A$ and $B$ are matrices, then usually $e^{A} e^{B} \neq e^{A + B}$. The following theorem provides a condition for this equality to hold. 

\begin{theorem}
Let $A, B$ be $n \times n$ matrices. If $A$ and $B$ are commuting matrices ($AB = BA$), then 
$$e^{A + B} = e^{A} e^{B}$$
\end{theorem}

\begin{proof}
If $AB = BA$, we use the definition of the matrix exponential to write $Ae^{Bt} = e^{Bt}A$, and similarly for other combinations of $A$, $B$, $A + B$, and their exponentials. 

Let $g(t) = e^{(A + B)t} e^{-Bt} e^{-At}$, where $t$ is a real scalar. By the previous theorem and the product rule for derivatives, we have 
$$
\begin{aligned}
g'(t) &= (A + B)e^{(A + B)t} e^{-Bt}e^{-At} + e^{(A + B)t} (-B) e^{-Bt} e^{-At} + e^{(A+B)t} e^{-Bt} (-A) e^{-At} \\
&= (A+B) g(t) - Bg(t) -Ag(t) \\
&= 0
\end{aligned}
$$

Note that we could only factor out $(-A)$ and $(-B)$ because we know $AB = BA$. 

Since the derivative is 0 for all $t$, $g(t)$ must be a matrix of constants, so $g(t) = C$, for some constant matrix $C$. Setting $t=0$ gives us 
$$C = g(0) = e^{(A+B)0} e^{-B0} e^{-A0} = I$$
which must also be true for all $t$ since $C$ is constant so 
$$e^{(A+B)t} e^{-Bt} e^{-At} = I$$
and by multiplying both sides by $e^{At} e^{Bt}$, we get 
$$e^{At} e^{Bt} = e^{(A+B)t}$$
\end{proof}